{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6662e2af-d0f0-47e9-88a2-dcbdf85e2a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8857dfd-513a-4433-a9b3-a3fbe1e6e250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.1 - Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ccfe86d-b788-4616-8ab1-51d09db367bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.1 Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8891e2-be97-4934-9410-ad3fc3ebf4e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Core Data Handling\n",
    "# %pip install pandas --quiet\n",
    "# %pip install numpy --quiet\n",
    "# %pip install openpyxl --quiet\n",
    "\n",
    "# # Modeling and Statistical Analysis\n",
    "# %pip install statsmodels --quiet\n",
    "# %pip install pygam --quiet\n",
    "\n",
    "# # Actuarial Modeling\n",
    "# %pip install chainladder --quiet\n",
    "# %pip install sparse==0.15.5 --quiet  # Newer version conflicts with chainladder\n",
    "\n",
    "# # Performance and Parallel Processing\n",
    "# %pip install swifter --quiet\n",
    "# %pip install joblib --quiet\n",
    "# %pip install tqdm --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdda476-63bb-4a78-87bc-c6a41a14cced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.2 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "565a5873-74f1-40e8-a13a-d31fa2154148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistical Modeling\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from pygam import PoissonGAM, GAM, s, f, te\n",
    "\n",
    "# Actuarial Modeling\n",
    "import chainladder as cl\n",
    "\n",
    "# Performance and Parallel Processing\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d966c734-b116-44cb-ae99-fd58d8386298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2 - Parameterisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc6f700-cc7c-44a2-8508-f65b0b1caca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters based on assumptions \n",
    "product = \"Private Motor\"\n",
    "development_term = 24\n",
    "last_day_previous_month = datetime(2024, 12, 31)\n",
    "last_n_month_lognormal = 12\n",
    "latest_balance_date_str = last_day_previous_month.strftime('%Y-%m-%d')\n",
    "\n",
    "print(latest_balance_date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407d2ed1-dc15-4fca-8a25-3abff179c603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f464a54-8bc7-4a6c-9b76-9ae6ea45ac8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.1 - Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de8702c-ee83-41d6-82a9-e01ea3298e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_claims = pd.read_feather('/Volumes/actuaries_prd/general/ibnr/data/claims_data.feather')\n",
    "data_exposure = pd.read_feather('/Volumes/actuaries_prd/general/ibnr/data/exposure_data.feather')\n",
    "data_gwp = pd.read_feather('/Volumes/actuaries_prd/general/ibnr/data/gwp_data.feather')\n",
    "data = pd.read_feather('/Volumes/actuaries_prd/general/ibnr/data/data_claims_exposure.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "736b15e3-78b0-40bc-82d1-07b7a6675cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2 - Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "418de623-d859-409d-a4ec-9f8d3aad4f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "data_date = '2024-12-31'\n",
    "development_period_end = 24\n",
    "n_periods = 12\n",
    "triangle_groups = ['channel', 'claim_type']\n",
    "acc_month_start = pd.to_datetime('2017-01-01')\n",
    "\n",
    "# Date Range\n",
    "valuation_dates = pd.date_range(start=\"2019-01-31\", end=data_date, freq='ME').strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "# Pre-filtering\n",
    "input_data = data[\n",
    "    (data['dev_month'] <= development_term) &\n",
    "    (data['acc_month'] >= acc_month_start)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e117841-06f2-44de-a88c-89d6bdff2362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76610c49-3bf6-4418-8a08-84ef0695c2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.1 - Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d03eef5-00a3-4126-976a-6f94cc02918f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_models(input_data, valuation_date, development_period_end = 24, triangle_groups = ['channel', 'claim_type'], n_periods = 12):\n",
    "\n",
    "    # 1. Data Processing\n",
    "    # =================================\n",
    "\n",
    "    data_hidden = input_data[\n",
    "        (input_data['obs_month'] <= valuation_date) &\n",
    "        (input_data['acc_month'] <= valuation_date)\n",
    "    ] \n",
    "\n",
    "    data_full = input_data.copy()\n",
    "\n",
    "    # Create triangle on partial data\n",
    "    triangle_combined = cl.Triangle(\n",
    "        data_hidden,\n",
    "        origin=\"acc_month\",\n",
    "        development=\"obs_month\",\n",
    "        columns=[\n",
    "            'claim_count',\n",
    "            'net_claim_incurred',\n",
    "            'gross_claim_incurred',\n",
    "            'net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed',\n",
    "            'earnprem_indexed',\n",
    "            'exposure',\n",
    "            'recoveries',\n",
    "            'earnprem'\n",
    "        ],\n",
    "        index=triangle_groups,\n",
    "        cumulative=False\n",
    "    ).incr_to_cum()\n",
    "\n",
    "    # Transformations\n",
    "    triangle_combined['frequency'] = triangle_combined['claim_count'] / triangle_combined['exposure']\n",
    "    triangle_combined['gross_cost_per_policy'] = triangle_combined['gross_claim_incurred'] / triangle_combined['exposure']\n",
    "    triangle_combined['gross_cost_per_policy_indexed'] = triangle_combined['gross_claim_incurred_indexed'] / triangle_combined['exposure']\n",
    "    triangle_combined['net_cost_per_policy'] = triangle_combined['net_claim_incurred'] / triangle_combined['exposure']\n",
    "    triangle_combined['net_cost_per_policy_indexed'] = triangle_combined['net_claim_incurred_indexed'] / triangle_combined['exposure']\n",
    "    triangle_combined['net_loss_ratio'] = triangle_combined['net_claim_incurred'] / triangle_combined['earnprem']\n",
    "    triangle_combined['net_loss_ratio_indexed'] = triangle_combined['net_claim_incurred_indexed'] / triangle_combined['earnprem_indexed']\n",
    "\n",
    "    # ------------------------------------\n",
    "    # FULL TRIANGLE (FOR ACTUAL RESULTS)\n",
    "    # ------------------------------------\n",
    "    triangle_combined_full = cl.Triangle(\n",
    "        data_full,\n",
    "        origin='acc_month',\n",
    "        development='obs_month',\n",
    "        columns=[\n",
    "            'claim_count',\n",
    "            'net_claim_incurred',\n",
    "            'gross_claim_incurred',\n",
    "            'net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed',\n",
    "            'earnprem_indexed',\n",
    "            'exposure',\n",
    "            'recoveries',\n",
    "            'earnprem'\n",
    "        ],\n",
    "        index=triangle_groups,\n",
    "        cumulative=False,\n",
    "    ).incr_to_cum()\n",
    "\n",
    "    output_actual_results = triangle_combined_full[\n",
    "        [\n",
    "            \"net_claim_incurred\",\n",
    "            \"gross_claim_incurred\",\n",
    "            \"claim_count\",\n",
    "            \"net_claim_incurred_indexed\",\n",
    "            \"gross_claim_incurred_indexed\",\n",
    "            \"recoveries_indexed\",\n",
    "        ]\n",
    "    ].latest_diagonal.to_frame().reset_index().drop(\n",
    "        columns=['valuation'] # Drop valuation and manually append after\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'origin' : 'acc_month',\n",
    "            'claim_count': 'latest_view_claim_count',\n",
    "            'net_claim_incurred': 'latest_view_net_claim_incurred',\n",
    "            'gross_claim_incurred': 'latest_view_gross_claim_incurred',\n",
    "            'recoveries': 'latest_view_recoveries',\n",
    "            'net_claim_incurred_indexed': 'latest_view_net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed': 'latest_view_gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed': 'latest_view_recoveries_indexed'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 2.1 Model Training - Claim Count\n",
    "    # =================================\n",
    "\n",
    "    # Build initial development triangle and replace the development factors to assume fully developed in {development_period_end} periods\n",
    "    claim_count_development_factors  = cl.Development(n_periods=n_periods).fit_transform(triangle_combined[['claim_count','frequency']])\n",
    "    for i in range(0, claim_count_development_factors.ldf_.values.shape[0]):\n",
    "        claim_count_development_factors.ldf_.values[i][:,0,development_period_end:] = 1\n",
    "        claim_count_development_factors.cdf_.values[i][:,0,development_period_end:] = 1\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 1 - Chainladder\n",
    "    # ------------------------------------\n",
    "    claim_count_chainladder = cl.Chainladder().fit(claim_count_development_factors)\n",
    "\n",
    "    weights = triangle_combined['exposure'].latest_diagonal\n",
    "    weights /= np.sum(weights, axis=2, keepdims=True)\n",
    "\n",
    "    apriori_claim_count = np.sum((claim_count_chainladder.ultimate_['frequency'] * weights).iloc[:, :, -12:, :], axis=2, keepdims=True) / np.sum(weights.iloc[:, :, -12:, :], axis=2, keepdims=True)\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 2 - Bornhuetter-Ferguson\n",
    "    # ------------------------------------\n",
    "    claim_count_bf = cl.BornhuetterFerguson(\n",
    "        apriori=  apriori_claim_count\n",
    "    ).fit(\n",
    "        triangle_combined[['claim_count','exposure']]\n",
    "        , sample_weight = triangle_combined['exposure'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 3 - Cape Cod\n",
    "    # ------------------------------------\n",
    "    claim_count_cc = cl.CapeCod().fit(\n",
    "        triangle_combined[['claim_count','exposure']],\n",
    "        sample_weight=triangle_combined['exposure'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    output_triangle = triangle_combined[\n",
    "        [\n",
    "            \"exposure\",\n",
    "            \"earnprem\",\n",
    "            \"earnprem_indexed\",\n",
    "            \"claim_count\",\n",
    "            \"net_claim_incurred\",\n",
    "            \"gross_claim_incurred\",\n",
    "            \"recoveries\",\n",
    "            \"net_claim_incurred_indexed\",\n",
    "            \"gross_claim_incurred_indexed\",\n",
    "            \"recoveries_indexed\",\n",
    "        ]\n",
    "    ].latest_diagonal.to_frame().reset_index().drop(\n",
    "        columns=['valuation']\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'origin' : 'acc_month',\n",
    "            'claim_count': 'reported_to_date_claim_count',\n",
    "            'net_claim_incurred': 'reported_to_date_net_claim_incurred',\n",
    "            'gross_claim_incurred': 'reported_to_date_gross_claim_incurred',\n",
    "            'recoveries': 'reported_to_date_recoveries',\n",
    "            'net_claim_incurred_indexed': 'reported_to_date_net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed': 'reported_to_date_gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed': 'reported_to_date_recoveries_indexed'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 2.2 Model Training - Net Incurred\n",
    "    # =================================\n",
    "\n",
    "    # Build initial development triangle and replace the development factors to assume fully developed in {development_period_end} periods\n",
    "    net_incurred_development_factors  = cl.Development(n_periods=n_periods).fit_transform(triangle_combined[['net_claim_incurred','net_cost_per_policy','net_loss_ratio']])\n",
    "    for i in range(0, net_incurred_development_factors.ldf_.values.shape[0]):\n",
    "        net_incurred_development_factors.ldf_.values[i][:,0,development_period_end:] = 1\n",
    "        net_incurred_development_factors.cdf_.values[i][:,0,development_period_end:] = 1\n",
    "\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 1 - Chainladder\n",
    "    # ------------------------------------\n",
    "    \n",
    "    # Fit a chainladder model using the adjusted development factors from dev object\n",
    "    net_incurred_chainladder = cl.Chainladder().fit(net_incurred_development_factors)\n",
    "\n",
    "    weights = triangle_combined['earnprem'].latest_diagonal\n",
    "    weights /= np.sum(weights, axis=2, keepdims=True)\n",
    "\n",
    "    apriori_net_incurred = np.sum((net_incurred_chainladder.ultimate_['net_loss_ratio'] * weights).iloc[:, :, -12:, :], axis=2, keepdims=True) / np.sum(weights.iloc[:, :, -12:, :], axis=2, keepdims=True)\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 2 - Bornhuetter-Ferguson\n",
    "    # ------------------------------------\n",
    "\n",
    "    net_incurred_bf = cl.BornhuetterFerguson(\n",
    "        apriori=apriori_net_incurred\n",
    "    ).fit(\n",
    "        net_incurred_development_factors, \n",
    "        sample_weight=triangle_combined['earnprem'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 3 - Cape Cod\n",
    "    # ------------------------------------\n",
    "\n",
    "    net_incurred_cc = cl.CapeCod().fit(\n",
    "        net_incurred_development_factors,  # Use the same dev factors!\n",
    "        sample_weight=triangle_combined['earnprem'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    # 3. Joins to Final Dataframe\n",
    "    # =================================\n",
    "\n",
    "    # ------------------------------------\n",
    "    # FULL TRIANGLE - ACTUAL RESULTS\n",
    "    # ------------------------------------\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_triangle,\n",
    "        output_actual_results,\n",
    "        left_on  = triangle_groups + ['acc_month'],\n",
    "        right_on =  triangle_groups + ['acc_month'],\n",
    "        how='left'\n",
    "    ).drop(\n",
    "        columns=['exposure']\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # COUNT - CHAIN LADDER\n",
    "    # ------------------------------------\n",
    "    output_claim_count_chainladder = claim_count_chainladder.ultimate_.to_frame().reset_index(\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'claim_count': 'ultimate_claim_count_chainladder'\n",
    "            ,'origin' : 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_claim_count_chainladder,\n",
    "        left_on  = triangle_groups + ['acc_month'],\n",
    "        right_on =  triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # NET INCURRED - CHAIN LADDER\n",
    "    # ------------------------------------\n",
    "\n",
    "    output_net_incurred_chainladder = net_incurred_chainladder.ultimate_.to_frame().reset_index().drop(\n",
    "        columns=['valuation', 'net_cost_per_policy']\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'net_claim_incurred': 'ultimate_net_incurred_chainladder',\n",
    "            'origin': 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_net_incurred_chainladder,\n",
    "        left_on = triangle_groups + ['acc_month'],\n",
    "        right_on = triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # COUNT - BORNHUETTER-FERGUSON\n",
    "    # ------------------------------------\n",
    "    output_claim_count_bf = claim_count_bf.ultimate_.to_frame().reset_index(\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'claim_count': 'ultimate_claim_count_bf'\n",
    "            ,'origin' : 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_claim_count_bf,\n",
    "        left_on  = triangle_groups + ['acc_month'],\n",
    "        right_on =  triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # NET INCURRED - BORNHUETTER-FERGUSON\n",
    "    # ------------------------------------\n",
    "\n",
    "    output_net_incurred_bf = net_incurred_bf.ultimate_.to_frame().reset_index().drop(\n",
    "        columns=['valuation', 'net_cost_per_policy', 'net_loss_ratio']\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'net_claim_incurred': 'ultimate_net_incurred_bf',\n",
    "            'origin': 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_net_incurred_bf,\n",
    "        left_on = triangle_groups + ['acc_month'],\n",
    "        right_on = triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # COUNT - CAPE COD\n",
    "    # ------------------------------------\n",
    "    output_claim_count_cc = claim_count_cc.ultimate_.to_frame().reset_index().rename(\n",
    "        columns={\n",
    "            'claim_count': 'ultimate_claim_count_cc',\n",
    "            'origin': 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_claim_count_cc,\n",
    "        left_on=triangle_groups + ['acc_month'],\n",
    "        right_on=triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # NET INCURRED - CAPE COD\n",
    "    # ------------------------------------\n",
    "\n",
    "    output_net_incurred_cc = net_incurred_cc.ultimate_.to_frame().reset_index().drop(\n",
    "        columns=['valuation', 'net_cost_per_policy', 'net_loss_ratio']\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'net_claim_incurred': 'ultimate_net_incurred_cc',\n",
    "            'origin': 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_net_incurred_cc,\n",
    "        left_on=triangle_groups + ['acc_month'],\n",
    "        right_on=triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    # Final Output\n",
    "    output_results['valuation_date'] = pd.to_datetime(valuation_date)\n",
    "    return output_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba86bb0b-f0e5-4cdd-bd07-bcb78e7aacf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.2 - Parallel Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420777bd-1866-42cc-befd-949764da5999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    # Run in parallel\n",
    "    valuation_data = Parallel(n_jobs=-1)(delayed(\n",
    "        build_models\n",
    "    )(\n",
    "        input_data,\n",
    "        date,\n",
    "        24,\n",
    "        ['channel', 'claim_type'],\n",
    "        12\n",
    "    ) for date in tqdm(valuation_dates, desc=\"Processing valuation dates\"))\n",
    "\n",
    "    # Combine results into a single DataFrame\n",
    "    combined_df = pd.concat(valuation_data, ignore_index=True)\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790c6bea-f750-4535-aebf-3b00374ab028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.3 - Post-shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f168e4d-6156-4c06-a2e9-e26e46e393e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reshape_forecast_output(\n",
    "    df: pd.DataFrame,\n",
    "    response_prefixes: dict = {\n",
    "        'ultimate_claim_count': {\n",
    "            'response_type': 'count',\n",
    "            'actual_col': 'latest_view_claim_count',\n",
    "            'reported_col': 'reported_to_date_claim_count'\n",
    "        },\n",
    "        'ultimate_net_incurred': {\n",
    "            'response_type': 'net_incurred',\n",
    "            'actual_col': 'latest_view_net_claim_incurred',\n",
    "            'reported_col': 'reported_to_date_net_claim_incurred'\n",
    "        }\n",
    "    },\n",
    "    id_columns: list = ['acc_month', 'valuation_date', 'channel', 'claim_type']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reshapes ultimate model output DataFrame into long format with actual and predicted values.\n",
    "    Handles both claim count and net incurred amounts.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame.\n",
    "    - response_prefixes: Dictionary mapping response prefixes to their metadata including:\n",
    "        - response_type: Label for type of response ('count', 'net_incurred', etc.)\n",
    "        - actual_col: Column name representing actual value\n",
    "        - reported_col: Column name representing reported-to-date value\n",
    "    - id_columns: List of identifying columns to retain (e.g. ['acc_month', 'valuation_date', ...]).\n",
    "\n",
    "    Returns:\n",
    "    - A tidy DataFrame with columns: id_columns + ['model', 'actual', 'predicted', 'latest_view_*', 'reported_to_date_*', 'response']\n",
    "    \"\"\"\n",
    "    result_dfs = []\n",
    "\n",
    "    # Process each response prefix type\n",
    "    for response_prefix, config in response_prefixes.items():\n",
    "        response_type = config['response_type']\n",
    "        actual_col = config['actual_col']\n",
    "        reported_col = config['reported_col']\n",
    "        \n",
    "        # Skip if required columns are not in the DataFrame\n",
    "        if actual_col not in df.columns or reported_col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Identify ultimate columns for the given response prefix\n",
    "        ultimate_cols = [col for col in df.columns if col.startswith(response_prefix)]\n",
    "        \n",
    "        if not ultimate_cols:\n",
    "            continue\n",
    "            \n",
    "        models = [col.split('_')[-1] for col in ultimate_cols]\n",
    "\n",
    "        for model, col_name in zip(models, ultimate_cols):\n",
    "            # Make sure all required columns exist before proceeding\n",
    "            required_cols = id_columns + [col_name, actual_col, reported_col]\n",
    "            if not all(col in df.columns for col in required_cols):\n",
    "                continue\n",
    "                \n",
    "            model_df = df[required_cols].copy()\n",
    "\n",
    "            model_df['model'] = model\n",
    "            model_df['actual'] = model_df[actual_col] - model_df[reported_col]\n",
    "            model_df['predicted'] = model_df[col_name] - model_df[reported_col]\n",
    "            model_df['response'] = response_type\n",
    "\n",
    "            # Rename columns to standardised names to ensure they're consistent\n",
    "            model_df = model_df.rename(columns={\n",
    "                actual_col: 'latest_view',\n",
    "                reported_col: 'reported_to_date'\n",
    "            })\n",
    "\n",
    "            # Drop the original ultimate column\n",
    "            model_df = model_df.drop(columns=[col_name])\n",
    "            result_dfs.append(model_df)\n",
    "\n",
    "    # If no results were found, return empty DataFrame with correct columns\n",
    "    if not result_dfs:\n",
    "        return pd.DataFrame(columns=id_columns + ['model', 'actual', 'predicted', 'latest_view', 'reported_to_date', 'response'])\n",
    "    \n",
    "    # Combine all results\n",
    "    result = pd.concat(result_dfs, ignore_index=True)\n",
    "    \n",
    "    # Ensure consistent column order\n",
    "    result = result[id_columns + ['model', 'actual', 'predicted', 'latest_view', 'reported_to_date', 'response']]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b898c5bc-ed5c-4def-aba2-6b6171542402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = reshape_forecast_output(\n",
    "    df=combined_df,\n",
    "    response_prefixes={\n",
    "        'ultimate_claim_count': {\n",
    "            'response_type': 'count',\n",
    "            'actual_col': 'latest_view_claim_count',\n",
    "            'reported_col': 'reported_to_date_claim_count'\n",
    "        },\n",
    "        'ultimate_net_incurred': {\n",
    "            'response_type': 'net_incurred',\n",
    "            'actual_col': 'latest_view_net_claim_incurred',\n",
    "            'reported_col': 'reported_to_date_net_claim_incurred'\n",
    "        }\n",
    "    },\n",
    "    id_columns=['acc_month', 'valuation_date', 'channel', 'claim_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d7f00d1-226a-4950-b262-b0b9fefe3535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.4 - Method Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c3754f81-882f-48c7-88f0-63a1ec652ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mae_excl_last_6_months(data, actual_col, pred_cols, groupby_cols, date_col='valuation_date'):\n",
    "    results = []\n",
    "\n",
    "    for keys, group in data.groupby(groupby_cols):\n",
    "        # Ensure date is datetime\n",
    "        group[date_col] = pd.to_datetime(group[date_col])\n",
    "\n",
    "        # Aggregate actual and predicted per valuation_date\n",
    "        agg_list = {'valuation_date': group[date_col].unique()}\n",
    "        agg_df = pd.DataFrame({'valuation_date': group[date_col]})\n",
    "        agg_df[actual_col] = group[actual_col]\n",
    "        for col in pred_cols:\n",
    "            agg_df[col] = group[col]\n",
    "\n",
    "        agg_df = agg_df.groupby('valuation_date').sum().reset_index()\n",
    "\n",
    "        # Define cutoff date\n",
    "        cutoff_date = agg_df['valuation_date'].max() - pd.DateOffset(months=6)\n",
    "        filtered_df = agg_df[agg_df['valuation_date'] <= cutoff_date]\n",
    "\n",
    "        # Compute MAE per model\n",
    "        mae_dict = {}\n",
    "        for col in pred_cols:\n",
    "            mae = np.abs(filtered_df[actual_col] - filtered_df[col]).mean()\n",
    "            mae_dict[col] = mae\n",
    "\n",
    "        if not mae_dict:\n",
    "            continue  # Skip if no models were evaluated\n",
    "\n",
    "        # Find best model\n",
    "        best_model = min(mae_dict, key=mae_dict.get)\n",
    "\n",
    "        # Save results\n",
    "        results.append({\n",
    "            **dict(zip(groupby_cols, keys if isinstance(keys, tuple) else [keys])),\n",
    "            **{f'mae_{col}': val for col, val in mae_dict.items()},\n",
    "            'best_model': best_model\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5452957c-3a4b-4879-86c9-3ab05b1212f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "072588a5-86d7-4197-aab1-97e48807d739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.4.1 - Claim Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398dee48-0af5-4e6d-8504-84013d9628b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Claim Count Method Selection\n",
    "claim_count_best_method = calculate_mae_excl_last_6_months(\n",
    "    combined_df,\n",
    "    actual_col='latest_view_claim_count',\n",
    "    pred_cols=[\n",
    "        'ultimate_claim_count_chainladder',\n",
    "        'ultimate_claim_count_bf',\n",
    "        'ultimate_claim_count_cc'\n",
    "    ],\n",
    "    # groupby_cols=['channel', 'claim_type']\n",
    "    groupby_cols=['channel'],\n",
    "    date_col='valuation_date'\n",
    ")\n",
    "\n",
    "claim_count_best_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2692b840-2b9e-4d0a-9ea3-e23c076ea55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.4.2 - Net Incurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbca5e48-0032-4c55-89a2-917bd182cf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Net Incurred Method Selection\n",
    "net_incurred_best_method = calculate_mae_excl_last_6_months(\n",
    "    combined_df,\n",
    "    actual_col='latest_view_net_claim_incurred',\n",
    "    pred_cols=[\n",
    "        'ultimate_net_incurred_chainladder',\n",
    "        'ultimate_net_incurred_bf',\n",
    "        'ultimate_net_incurred_cc'\n",
    "    ],\n",
    "    # groupby_cols=['channel', 'claim_type']\n",
    "    groupby_cols=['channel'],\n",
    "    date_col='valuation_date'\n",
    ")\n",
    "\n",
    "net_incurred_best_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1597153-2ee1-45a1-bfe6-dad2cc628ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c97baf7-619e-4ff1-af40-d1c92e4ce5fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.1 - Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb766fe-714a-4b21-b97a-cc5ef7630998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "def plot_claims_AVE(output, mode='net_incurred', risk_class=None, channel=None, prem_class=None, claim_type=None, models=None):\n",
    "    \"\"\"\n",
    "    Plot claims Average vs Expected with updated filtering options and MAE table.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    output : pandas DataFrame\n",
    "        The dataset containing claims data\n",
    "    mode : str, optional\n",
    "        'count' or 'net_incurred', defaults to 'net_incurred'\n",
    "    risk_class : str, optional\n",
    "        Filter by specific risk class\n",
    "    prem_class : str, optional\n",
    "        Filter by specific premium class\n",
    "    claim_type : str, optional\n",
    "        Filter by specific claim type\n",
    "    models : list, optional\n",
    "        List of models to include\n",
    "    \"\"\"\n",
    "    if mode not in ['count', 'net_incurred']:\n",
    "        raise ValueError(\"Invalid mode. Available modes are: 'count', 'net_incurred'.\")\n",
    "    \n",
    "    # Suppress warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Filter data\n",
    "    filtered_data = output[(output['response'] == mode)].copy()\n",
    "\n",
    "    # Apply new filtering options\n",
    "    if risk_class:\n",
    "        filtered_data = filtered_data[filtered_data['risk_class'] == risk_class]\n",
    "    if channel:\n",
    "        filtered_data = filtered_data[filtered_data['channel'] == channel]\n",
    "    if prem_class:\n",
    "        filtered_data = filtered_data[filtered_data['prem_class'] == prem_class]\n",
    "    if claim_type:\n",
    "        filtered_data = filtered_data[filtered_data['claim_type'] == claim_type]\n",
    "    if models:\n",
    "        filtered_data = filtered_data[filtered_data['model'].isin(models)]\n",
    "    \n",
    "    # Convert balance month to datetime\n",
    "    filtered_data['valuation_date'] = pd.to_datetime(filtered_data['valuation_date'])\n",
    "    \n",
    "    # Group by balance month and model, calculate sum of actual and predicted\n",
    "    grouped_data = filtered_data.groupby(['valuation_date', 'model']).agg({\n",
    "        'actual': 'sum',\n",
    "        'predicted': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Extract actual values (taking first model's values for reference)\n",
    "    if not grouped_data.empty and len(grouped_data['model'].unique()) > 0:\n",
    "        first_model = grouped_data['model'].unique()[0]\n",
    "        actual_values = grouped_data[grouped_data['model'] == first_model][['valuation_date', 'actual']]\n",
    "    else:\n",
    "        print(\"No data available for the selected filters.\")\n",
    "        return\n",
    "    \n",
    "    # Define cutoff date for MAE calculation (6 months from the latest date)\n",
    "    latest_date = grouped_data['valuation_date'].max()\n",
    "    cutoff_date = latest_date - pd.DateOffset(months=6)\n",
    "    \n",
    "    # Calculate MAE excluding last 6 months\n",
    "    mae_results = {}\n",
    "    for model in grouped_data['model'].unique():\n",
    "        model_data = grouped_data[grouped_data['model'] == model]\n",
    "        mae_data = model_data[model_data['valuation_date'] <= cutoff_date]\n",
    "        if not mae_data.empty:\n",
    "            mae = abs(mae_data['actual'] - mae_data['predicted']).mean()\n",
    "            mae_results[model] = mae\n",
    "    \n",
    "    # Create a subplot with 2 rows - one for the chart and one for the table\n",
    "    # Increase vertical spacing for more padding between chart and table\n",
    "    fig = make_subplots(\n",
    "        rows=2, \n",
    "        cols=1,\n",
    "        row_heights=[0.75, 0.25],  # Adjusted to give more space to the table\n",
    "        vertical_spacing=0.15,      # Increased for more padding\n",
    "        specs=[[{\"type\": \"scatter\"}], [{\"type\": \"table\"}]]\n",
    "    )\n",
    "    \n",
    "    # Add actual values trace to the first subplot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=actual_values['valuation_date'], \n",
    "            y=actual_values['actual'], \n",
    "            mode='lines+markers', \n",
    "            name='Actual',\n",
    "            line=dict(color='black', width=2),\n",
    "            hovertemplate='%{x}<br>Actual: %{y:.2f}'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Define color mapping for consistent colors across models\n",
    "    model_colors = {\n",
    "        'chainladder': 'blue',\n",
    "        'bornhuetter-ferguson': 'red',\n",
    "        'cape-cod': 'green',\n",
    "        'munich': 'purple'\n",
    "    }\n",
    "    \n",
    "    # Add predicted values traces for each model to the first subplot\n",
    "    for model in grouped_data['model'].unique():\n",
    "        model_data = grouped_data[grouped_data['model'] == model]\n",
    "        color = model_colors.get(model, None)  # Get color from mapping if available\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_data['valuation_date'], \n",
    "                y=model_data['predicted'], \n",
    "                mode='lines+markers', \n",
    "                name=f'ibnr - {model}',\n",
    "                line=dict(color=color) if color else {},  # Apply color if defined\n",
    "                hovertemplate='%{x}<br>Predicted: %{y:.2f}'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Build title with filtering information\n",
    "    title_parts = [f'Claim {mode.capitalize()}: Actual vs Predicted']\n",
    "    if risk_class:\n",
    "        title_parts.append(f'Risk Class: {risk_class}')\n",
    "    if channel:\n",
    "        title_parts.append(f'Channel: {channel}')\n",
    "    if prem_class:\n",
    "        title_parts.append(f'Premium Class: {prem_class}')\n",
    "    if claim_type:\n",
    "        title_parts.append(f'Claim Type: {claim_type}')\n",
    "    title = ' | '.join(title_parts)\n",
    "\n",
    "    # Create a table for MAE results, rankings and filters\n",
    "    # Sort models by MAE to determine ranking\n",
    "    ranked_models = sorted([(model, mae) for model, mae in mae_results.items()], key=lambda x: x[1])\n",
    "    \n",
    "    # Create ordered lists for the table\n",
    "    model_names = [model for model, _ in ranked_models]\n",
    "    mae_values = [f\"{mae:.2f}\" for _, mae in ranked_models]\n",
    "    # Create rankings (1 to n)\n",
    "    rankings = [f\"#{i+1}\" for i in range(len(ranked_models))]\n",
    "    \n",
    "    # Prepare filter information for the table\n",
    "    filter_names = []\n",
    "    filter_values = []\n",
    "    \n",
    "    # Add mode to filter info\n",
    "    filter_names.append(\"Mode\")\n",
    "    filter_values.append(mode)\n",
    "    \n",
    "    # Add other filters if they're set\n",
    "    if risk_class:\n",
    "        filter_names.append(\"Risk Class\")\n",
    "        filter_values.append(risk_class)\n",
    "    if channel:\n",
    "        filter_names.append(\"Channel\")\n",
    "        filter_values.append(channel)\n",
    "    if prem_class:\n",
    "        filter_names.append(\"Premium Class\")\n",
    "        filter_values.append(prem_class)\n",
    "    if claim_type:\n",
    "        filter_names.append(\"Claim Type\")\n",
    "        filter_values.append(claim_type)\n",
    "    if models:\n",
    "        filter_names.append(\"Models\")\n",
    "        filter_values.append(\", \".join(models) if isinstance(models, list) else models)\n",
    "    \n",
    "    # Add cutoff date information\n",
    "    filter_names.append(\"MAE Cutoff Date\")\n",
    "    filter_values.append(cutoff_date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    # Add MAE table to the second subplot - now including ranking column\n",
    "    fig.add_trace(\n",
    "        go.Table(\n",
    "            header=dict(\n",
    "                values=['Ranking', 'Model', 'MAE (excluding last 6 months)', 'Filter', 'Value'],\n",
    "                fill_color='paleturquoise',\n",
    "                align='left',\n",
    "                font=dict(size=12)\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[\n",
    "                    rankings + [\"\"] * (len(filter_names) - len(rankings)) if len(filter_names) > len(rankings) else rankings,\n",
    "                    model_names + [\"\"] * (len(filter_names) - len(model_names)) if len(filter_names) > len(model_names) else model_names,\n",
    "                    mae_values + [\"\"] * (len(filter_names) - len(mae_values)) if len(filter_names) > len(mae_values) else mae_values,\n",
    "                    filter_names + [\"\"] * (len(model_names) - len(filter_names)) if len(model_names) > len(filter_names) else filter_names,\n",
    "                    filter_values + [\"\"] * (len(model_names) - len(filter_values)) if len(model_names) > len(filter_values) else filter_values\n",
    "                ],\n",
    "                fill_color=[\n",
    "                    ['lavender'] * len(rankings) + ['white'] * (len(filter_names) - len(rankings)) if len(filter_names) > len(rankings) else ['lavender'] * len(rankings),\n",
    "                    ['lavender'] * len(model_names) + ['white'] * (len(filter_names) - len(model_names)) if len(filter_names) > len(model_names) else ['lavender'] * len(model_names),\n",
    "                    ['lavender'] * len(mae_values) + ['white'] * (len(filter_names) - len(mae_values)) if len(filter_names) > len(mae_values) else ['lavender'] * len(mae_values),\n",
    "                    ['ghostwhite'] * len(filter_names) + ['white'] * (len(model_names) - len(filter_names)) if len(model_names) > len(filter_names) else ['ghostwhite'] * len(filter_names),\n",
    "                    ['ghostwhite'] * len(filter_values) + ['white'] * (len(model_names) - len(filter_values)) if len(model_names) > len(filter_values) else ['ghostwhite'] * len(filter_values)\n",
    "                ],\n",
    "                align='left',\n",
    "                font=dict(size=11)\n",
    "            )\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Add zero line to the first subplot\n",
    "    min_date = actual_values['valuation_date'].min()\n",
    "    max_date = actual_values['valuation_date'].max()\n",
    "    \n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=min_date,\n",
    "        y0=0,\n",
    "        x1=max_date,\n",
    "        y1=0,\n",
    "        line=dict(color=\"gray\", width=1, dash=\"dash\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout with more height and adjusted margins\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=800,  # Increased height to better accommodate the expanded table\n",
    "        margin=dict(t=100, b=50, l=50, r=50),\n",
    "        template='plotly_white',\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update xaxis and yaxis for the first subplot\n",
    "    fig.update_xaxes(\n",
    "        title_text='Valuation Date',\n",
    "        tickangle=-45,\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(\n",
    "        title_text=f'Claim {mode.capitalize()}',\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add annotation for best model at the bottom left of the chart\n",
    "    if len(mae_results) > 1:\n",
    "        best_model = ranked_models[0][0]\n",
    "        best_mae = ranked_models[0][1]\n",
    "        fig.add_annotation(\n",
    "            x=0.07,  # Position at the bottom left (%)\n",
    "            y=0.45,  # Position at the bottom left (%)\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            text=f\"Best model: {best_model} (MAE: {best_mae:.2f})\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=11, color=\"green\"),\n",
    "            align=\"left\",\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.8)\",  # Semi-transparent white background\n",
    "            bordercolor=\"green\",\n",
    "            borderwidth=1,\n",
    "            borderpad=4,\n",
    "            xanchor=\"left\",\n",
    "            yanchor=\"bottom\"\n",
    "        )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "748489f0-17e1-461e-8ec7-a09f988a7872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.2 - Claim Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f374ba4-fb37-4ada-bbb6-2536687238bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get unique channels from the DataFrame\n",
    "channels_list = result[\"channel\"].dropna().unique()\n",
    "\n",
    "# Loop through each channel and plot claim count\n",
    "for channel in channels_list:\n",
    "    plot_claims_AVE(\n",
    "        result,\n",
    "        mode='count',\n",
    "        channel=channel\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b341472-f992-4863-b8c3-6269f80abbe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.3 - Net Incurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "373b05cc-e061-416a-bdbb-abaecf54080f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get unique channels from the DataFrame\n",
    "channels_list = result[\"channel\"].dropna().unique()\n",
    "\n",
    "# Loop through each channel and plot net incurred\n",
    "for channel in channels_list:\n",
    "    plot_claims_AVE(\n",
    "        result,\n",
    "        mode='net_incurred',\n",
    "        channel=channel\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IBNR Modelling Template",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
