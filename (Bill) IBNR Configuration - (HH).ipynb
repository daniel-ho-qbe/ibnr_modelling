{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d6599b-68f7-48df-ae23-00eed6f34b4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Global Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18dfb876-ef8d-438b-bbeb-1290df6f9378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.1 - Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2c068a-f9d5-4b9b-ad8f-98775350a425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.1 - Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d0e8e72d-ed1e-4f6e-a148-985d9e2f5732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Core Data Handling\n",
    "%pip install pandas --quiet\n",
    "%pip install numpy --quiet\n",
    "%pip install openpyxl --quiet\n",
    "\n",
    "# Modeling and Statistical Analysis\n",
    "%pip install statsmodels --quiet\n",
    "%pip install pygam --quiet\n",
    "\n",
    "# Actuarial Modelling\n",
    "%pip install chainladder --quiet\n",
    "%pip install sparse==0.15.5 --quiet  # Newer version conflicts with chainladder\n",
    "\n",
    "# Performance and Parallel Processing\n",
    "%pip install swifter --quiet\n",
    "%pip install joblib --quiet\n",
    "%pip install tqdm --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "589ac2c4-9cbd-4390-aba3-c35f5ea87a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.2 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "291cd156-d73d-4b73-8da8-0c1672f25628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import warnings\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistical Modelling\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from pygam import PoissonGAM, GAM, s, f, te\n",
    "\n",
    "# Actuarial Modelling\n",
    "import chainladder as cl\n",
    "\n",
    "# Performance and Parallel Processing\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3792fbfc-1f32-445a-b1a8-3718d11ce0b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.3 - Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1f152f-0f19-4606-9d42-ed8e0ce545a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Modelling Parameters\n",
    "development_term = 24\n",
    "last_day_previous_month = datetime(2025, 5, 31)\n",
    "last_n_month_lognormal = 12\n",
    "latest_balance_date_str = last_day_previous_month.strftime('%Y-%m-%d')\n",
    "\n",
    "exclude_last_n_month = 6\n",
    "acc_month_start = pd.to_datetime('2017-01-01')\n",
    "valuation_dates = pd.date_range(start=\"2019-01-31\", end=last_day_previous_month - relativedelta(months=exclude_last_n_month), freq='ME').strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "# Indexation\n",
    "cpi_file_path = '/Volumes/actuaries_prd/general/ibnr/enrichment/cpi.csv'\n",
    "\n",
    "cpi_by_quarter = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        Date,\n",
    "        CPI as cpi,\n",
    "        CONCAT(YEAR(Date), 'Q', QUARTER(Date)) AS quarter\n",
    "    FROM actuaries_prd.reference_data.abs_quarterly_cpi\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b58486b-9500-44fb-9391-6902bf8bac10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Collection for loop\n",
    "product_configs = {}\n",
    "\n",
    "# Function to add (if product does not exist) or update (if product exists) configs\n",
    "def update_product_config(config_dict, product_name, input_product_short=None, input_claim_data=None, input_expo_data=None, input_gwp_data=None, input_model_data=None, input_main_level=None, input_sub_levels=None):\n",
    "    # If product doesn't exist, initialise it\n",
    "    if product_name not in config_dict:\n",
    "        config_dict[product_name] = {}\n",
    "\n",
    "    # Update only the parameters that are provided\n",
    "    if input_product_short is not None:\n",
    "        config_dict[product_name][\"product_short\"] = input_product_short\n",
    "    if input_claim_data is not None:\n",
    "        config_dict[product_name][\"claim_data\"] = input_claim_data\n",
    "    if input_expo_data is not None:\n",
    "        config_dict[product_name][\"expo_data\"] = input_expo_data\n",
    "    if input_gwp_data is not None:\n",
    "        config_dict[product_name][\"gwp_data\"] = input_gwp_data\n",
    "    if input_model_data is not None:\n",
    "        config_dict[product_name][\"model_data\"] = input_model_data\n",
    "    if input_main_level is not None:\n",
    "        config_dict[product_name][\"main_level\"] = input_main_level\n",
    "    if input_sub_levels is not None:\n",
    "        config_dict[product_name][\"sub_levels\"] = input_sub_levels\n",
    "\n",
    "# Function to check product_configs\n",
    "def print_product_configs_summary(config_dict):\n",
    "    for product, config in config_dict.items():\n",
    "        print(f\"\\n Product: {product}\")\n",
    "        for key, value in config.items():\n",
    "            if isinstance(value, pd.DataFrame):\n",
    "                print(f\"  - {key}: DataFrame with shape {value.shape}\")\n",
    "            else:\n",
    "                print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e622484-97d2-4c1d-9078-57809d38fc5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b441afa-15ea-4303-b1c3-b2608a1310fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2 - Householders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a24684-f188-4dba-ae27-52bd0dbd680a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2.1 - App Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601712cc-d6b5-4165-8469-50dffa66b38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Product Name\n",
    "product = 'Householders'\n",
    "product_short = 'HH'\n",
    "folder_path = '' # path needs to be valid\n",
    "# add version folder path for version management to product configurations\n",
    "\n",
    "# Aggregation Levels ## Configuring for app parameters (This will impact the current adequacy summary table results.lji It will be aggregated to the main level)\n",
    "main_level = 'channel' # or segment (could be None)\n",
    "sub_levels = ['premcls', 'claim_type'] # (could be None), no aggregation but one adequacy result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf0ea4b7-465d-466c-ae03-63bf5296ad09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2.2 - Claim Data for IBNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "185c249e-f95f-42d3-ab21-51bcbc6caa93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "claim_data_for_ibnr = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('MM', g.loss_date) AS acc_month, \n",
    "        GREATEST(DATE_TRUNC('MM', a.observation_year_month),DATE_TRUNC('MM', g.loss_date)) AS obs_month,\n",
    "        greatest((YEAR(a.observation_year_month) - YEAR(g.loss_date)) * 12 + MONTH(a.observation_year_month) - MONTH(g.loss_date) + 1,1 ) AS dev_month,\n",
    "        p.ClaimType as claim_type,\n",
    "        g.ANZO_Super_Class AS product_group,\n",
    "        CASE WHEN coalesce(g.cell_name, 'Direct') = 'Householders' THEN 'Direct' else coalesce(g.cell_name, 'Direct') END AS channel,\n",
    "        -- PremClass as premcls,\n",
    "        SUM(a.new_claims_count) AS claim_count, \n",
    "        SUM(a.net_claims_incurred_movement_amount_gst_excl) AS net_claim_incurred,\n",
    "        SUM(a.gross_claims_incurred_movement_amount_gst_excl) as gross_claim_incurred,\n",
    "        SUM(a.claim_recoveries_movement_amount_gst_excl) as recoveries\n",
    "    FROM \n",
    "        cds_prd.cds.claim_claim_transactionmonth_financialcounts a \n",
    "    LEFT JOIN\n",
    "        cds_prd.rds.claim_claim_transactiondaily_financialcounts_detail g on a.claim_fkey = g.claim_origin_key\n",
    "    LEFT JOIN \n",
    "        ids_prd.ref.ref_cause_of_loss c ON a.cause_of_loss_fkey = c.origin_key\n",
    "    LEFT JOIN \n",
    "        (select distinct PolicyNumber,ReferenceProductCode_Ext FROM staging_prd.gw.pc_policyperiod) h ON g.Policy_Number = h.PolicyNumber\n",
    "    LEFT JOIN\n",
    "        actuarial_onprem_sqlserver.dbo.dim_hh_claim_premcls_18072025 p ON g.Claim_Number = RIGHT(p.CLAIMNO, LEN(p.CLAIMNO) - 1)\n",
    "    WHERE \n",
    "        YEAR(g.loss_date) >= 2017\n",
    "        AND g.anzo_super_class = '{product}'\n",
    "        AND a.observation_year_month <= '{latest_balance_date_str}'\n",
    "        AND g.loss_date <= '{latest_balance_date_str}'\n",
    "        AND c.incident_description is not null\n",
    "    GROUP BY \n",
    "        all\n",
    "    ORDER BY \n",
    "        all\n",
    "    \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe67783-52f8-4eb6-aeae-bfb1502bd3c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2.3 - Claim Data for Adequacy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4929da0d-f7e5-446c-a3a5-f2c3342e892a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "claim_data_for_adequacy_analysis = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('MM', g.loss_date) AS acc_month, \n",
    "        GREATEST(DATE_TRUNC('MM', a.observation_year_month),DATE_TRUNC('MM', g.loss_date)) AS obs_month,\n",
    "        greatest((YEAR(a.observation_year_month) - YEAR(g.loss_date)) * 12 + MONTH(a.observation_year_month) - MONTH(g.loss_date) + 1,1 ) AS dev_month,\n",
    "        p.ClaimType as claim_type,\n",
    "        g.ANZO_Super_Class AS product_group,\n",
    "        CASE WHEN coalesce(g.cell_name, 'Direct') = 'Householders' THEN 'Direct' else coalesce(g.cell_name, 'Direct') END AS channel,\n",
    "        PremClass as premcls,\n",
    "        SUM(a.new_claims_count) AS claim_count, \n",
    "        SUM(a.net_claims_incurred_movement_amount_gst_excl) AS net_claim_incurred,\n",
    "        SUM(a.gross_claims_incurred_movement_amount_gst_excl) as gross_claim_incurred,\n",
    "        SUM(a.claim_recoveries_movement_amount_gst_excl) as recoveries\n",
    "    FROM \n",
    "        cds_prd.cds.claim_claim_transactionmonth_financialcounts a \n",
    "    LEFT JOIN\n",
    "        cds_prd.rds.claim_claim_transactiondaily_financialcounts_detail g on a.claim_fkey = g.claim_origin_key\n",
    "    LEFT JOIN \n",
    "        ids_prd.ref.ref_cause_of_loss c ON a.cause_of_loss_fkey = c.origin_key\n",
    "    LEFT JOIN \n",
    "        (select distinct PolicyNumber,ReferenceProductCode_Ext FROM staging_prd.gw.pc_policyperiod) h ON g.Policy_Number = h.PolicyNumber\n",
    "    LEFT JOIN\n",
    "        actuarial_onprem_sqlserver.dbo.dim_hh_claim_premcls_18072025 p ON g.Claim_Number = RIGHT(p.CLAIMNO, LEN(p.CLAIMNO) - 1)\n",
    "    WHERE \n",
    "        YEAR(g.loss_date) >= 2017\n",
    "        AND g.anzo_super_class = '{product}'\n",
    "        AND a.observation_year_month <= '{latest_balance_date_str}'\n",
    "        AND g.loss_date <= '{latest_balance_date_str}'\n",
    "        AND c.incident_description is not null\n",
    "    GROUP BY \n",
    "        all\n",
    "    ORDER BY \n",
    "        all\n",
    "    \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "010ebb63-c55f-43d0-bb5c-cbf7011131bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2.3 - Exposure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c8c63c-978e-4be1-a666-fd37e5b0080b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expo_data = spark.sql(f\"\"\"\n",
    "    select\n",
    "        DATE_TRUNC('MM', exp_start) AS acc_month,\n",
    "        CASE \n",
    "            WHEN COALESCE(channel, 'Direct') = 'ANZ' THEN 'ANZ'\n",
    "            WHEN COALESCE(channel, 'Direct') = 'BD' THEN 'Broker Distribution'\n",
    "            WHEN COALESCE(channel, 'Direct') = 'ELDERS' THEN 'Elders'\n",
    "            WHEN COALESCE(channel, 'Direct') = 'FIOTHER' THEN 'FI Other'\n",
    "            WHEN COALESCE(channel, 'Direct') IN ('DIRECT', 'AUSPOST', 'KOGAN', 'Direct') THEN 'Direct'\n",
    "            ELSE COALESCE(channel, 'Direct')\n",
    "        END AS channel,\n",
    "        PREM_CLASS as premcls,\n",
    "        sum(EARNPREM) as earnprem,\n",
    "        sum(EXPOSURE) as exposure,\n",
    "        1 as dev_month\n",
    "    from actuarial_onprem_sqlserver.dbo.fact_hh_prem_premcls\n",
    "    where exp_start >= '2017-01-01' AND exp_start <= '{latest_balance_date_str}'\n",
    "    group by all\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0374c392-da49-4084-b784-5fc18f7a469b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2.4 - GWP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d2b317-000a-4523-81ed-ac5c6b2e98d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gwp_data = spark.sql(f\"\"\"SELECT COUNT(DISTINCT PolicyNumber) AS policy_count,\n",
    "  COUNT(*) AS risk_count,\n",
    "  SUM(`Premium Excl. GST`) as GWP,\n",
    "  CASE \n",
    "      WHEN COALESCE(channel, 'Direct') = 'ANZ' THEN 'ANZ'\n",
    "      WHEN COALESCE(channel, 'Direct') = 'BD' THEN 'Broker Distribution'\n",
    "      WHEN COALESCE(channel, 'Direct') = 'ELDERS' THEN 'Elders'\n",
    "      WHEN COALESCE(channel, 'Direct') IN ('FI', 'FIOTHER') THEN 'FI Other'\n",
    "      WHEN COALESCE(channel, 'Direct') IN ('DIRECT', 'Direct', 'Direct - Omni') THEN 'Direct'\n",
    "      ELSE COALESCE(channel, 'Direct')\n",
    "  END AS channel,\n",
    "  `Policy Inception Quarter` AS YearQuarter\n",
    "FROM actuarial_onprem_sqlserver.dbo.HH_Portfolio_Growth\n",
    "WHERE flag_quote = 1\n",
    "GROUP BY ALL\n",
    "ORDER BY ALL\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a29cd49-eebb-494f-ad4e-5e328ddbe5b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2.5 - Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab85613-5f19-4c2f-931a-c882b3785b18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Attach and clean for earnprem and exposure data\n",
    "data = pd.merge(\n",
    "    claim_data_for_adequacy_analysis,\n",
    "    expo_data,\n",
    "    on=['acc_month', 'dev_month', 'channel'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "data[['earnprem', 'exposure']] = data[['earnprem', 'exposure']].fillna(0)\n",
    "\n",
    "# Filter data input\n",
    "model_data = data[\n",
    "    (data['dev_month'] <= development_term) &\n",
    "    (data['acc_month'] >= acc_month_start)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f1878d-01bd-432d-abf9-159db3dc0158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd026d67-f69c-48d7-8bb7-82d364594fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "update_product_config(product_configs, product, input_product_short=product_short, input_claim_data=claim_data_for_adequacy_analysis, input_expo_data=expo_data, input_gwp_data=gwp_data, input_model_data=model_data, input_main_level=main_level, input_sub_levels=sub_levels)\n",
    "print_product_configs_summary(product_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb205b4-1d66-42f1-8495-c9502dd43917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Model Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d289de9-90e1-4103-9f33-17e59841309b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.1 - Product Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a9e991-87a6-4d14-a026-3ef2fdb7988a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# product_configs.pop(\"Private Motor\")\n",
    "print_product_configs_summary(product_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94932ac1-9cd5-4878-a234-a62a62bf6153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.2 - Model Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "401b2de3-0a5f-466c-a7cd-69377ab38e54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %run \"./(Bill) IBNR Modelling Template (Standardised)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "227b8fef-1d6b-455e-83b2-8458dd88ec54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74f85ee3-9166-4c84-a64f-0716de9d5aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Read the HTML file content\n",
    "# with open(\"/Workspace/Shared/General/IBNR project/ibnr_modelling/temp_output.html\", \"r\") as f:\n",
    "#     html_content = f.read()\n",
    "\n",
    "# # Display it in the notebook\n",
    "# displayHTML(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b1a9d6f-4db5-4516-a2ec-e1242f86036f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Application Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362d8a45-bdba-4101-b051-2086c28cd762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# product_configs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f87802-fc98-445d-a4f4-0a73f701efb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# app_config_rows = [\n",
    "#     {\n",
    "#         \"Product\": product,\n",
    "#         \"Table\": f\"actuaries_prd.general.{config.get('product_short', '').lower()}_ultimates_new\",\n",
    "#         \"Main_Level\": config.get('main_level', ''),\n",
    "#         \"Sub_Levels\": config.get('sub_levels', '')\n",
    "#     }\n",
    "#     for product, config in product_configs.items()\n",
    "# ]\n",
    "\n",
    "# spark.createDataFrame(app_config_rows) \\\n",
    "#     .write \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"overwriteSchema\", \"true\") \\\n",
    "#     .saveAsTable(\"actuaries_prd.general.ibnr_product_configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5d1230-69f6-441b-861b-919803e4f43a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# TEST SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ca6be29f-9687-4125-905c-133aa1d1ce2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_models_new(input_data, valuation_date, development_period_end=24, model_triangle_groups=['channel', 'claim_type'], output_triangle_groups=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - input_data: Combined claim and exposure data\n",
    "    - valuation_date: Valuation date\n",
    "    - development_period_end: Period where development is assumed complete\n",
    "    - model_triangle_groups: Grouping for building the models (less granular)\n",
    "    - output_triangle_groups: Grouping for final output (more granular, optional)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with results at the output granularity level\n",
    "    \"\"\"\n",
    "    # Default if output groups are not specified\n",
    "    if output_triangle_groups is None:\n",
    "        output_triangle_groups = model_triangle_groups\n",
    "\n",
    "    # Suppress all warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # 1. Data Processing\n",
    "    # =================================\n",
    "\n",
    "    data_hidden = input_data[\n",
    "        (input_data['obs_month'] <= valuation_date) &\n",
    "        (input_data['acc_month'] <= valuation_date)\n",
    "    ] \n",
    "\n",
    "    # Aggregate to model granularity for triangle building\n",
    "    model_data = data_hidden.groupby(\n",
    "        model_triangle_groups + ['acc_month', 'obs_month'], \n",
    "        as_index=False\n",
    "    ).agg({\n",
    "        'claim_count': 'sum',\n",
    "        'net_claim_incurred': 'sum',\n",
    "        'gross_claim_incurred': 'sum',\n",
    "        'net_claim_incurred_indexed': 'sum',\n",
    "        'gross_claim_incurred_indexed': 'sum',\n",
    "        'recoveries_indexed': 'sum',\n",
    "        'earnprem_indexed': 'sum',\n",
    "        'exposure': 'sum',\n",
    "        'recoveries': 'sum',\n",
    "        'earnprem': 'sum'\n",
    "    })\n",
    "    \n",
    "    # Create triangle for model building\n",
    "    triangle_combined = cl.Triangle(\n",
    "        model_data,\n",
    "        origin=\"acc_month\",\n",
    "        development=\"obs_month\",\n",
    "        columns=[\n",
    "            'claim_count',\n",
    "            'net_claim_incurred',\n",
    "            'gross_claim_incurred',\n",
    "            'net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed',\n",
    "            'earnprem_indexed',\n",
    "            'exposure',\n",
    "            'recoveries',\n",
    "            'earnprem'\n",
    "        ],\n",
    "        index=model_triangle_groups,\n",
    "        cumulative=False\n",
    "    ).incr_to_cum()\n",
    "\n",
    "    # Transformations\n",
    "    triangle_combined['frequency'] = triangle_combined['claim_count'] / triangle_combined['exposure']\n",
    "    triangle_combined['gross_cost_per_policy'] = triangle_combined['gross_claim_incurred'] / triangle_combined['exposure']\n",
    "    triangle_combined['gross_cost_per_policy_indexed'] = triangle_combined['gross_claim_incurred_indexed'] / triangle_combined['exposure']\n",
    "    triangle_combined['net_cost_per_policy'] = triangle_combined['net_claim_incurred'] / triangle_combined['exposure']\n",
    "    triangle_combined['net_cost_per_policy_indexed'] = triangle_combined['net_claim_incurred_indexed'] / triangle_combined['exposure']\n",
    "    triangle_combined['net_loss_ratio'] = triangle_combined['net_claim_incurred'] / triangle_combined['earnprem']\n",
    "    triangle_combined['net_loss_ratio_indexed'] = triangle_combined['net_claim_incurred_indexed'] / triangle_combined['earnprem_indexed']\n",
    "\n",
    "    # 2.1 Model Training - Claim Count\n",
    "    # =================================\n",
    "\n",
    "    claim_count_development_factors = cl.Development(n_periods=development_period_end).fit_transform(\n",
    "        triangle_combined[['claim_count','frequency']]\n",
    "    )\n",
    "    for i in range(0, claim_count_development_factors.ldf_.values.shape[0]):\n",
    "        claim_count_development_factors.ldf_.values[i][:,0,development_period_end:] = 1\n",
    "        claim_count_development_factors.cdf_.values[i][:,0,development_period_end:] = 1\n",
    "\n",
    "    # Chainladder\n",
    "    claim_count_chainladder = cl.Chainladder().fit(claim_count_development_factors)\n",
    "    \n",
    "    weights = triangle_combined['exposure'].latest_diagonal\n",
    "    weights /= np.sum(weights, axis=2, keepdims=True)\n",
    "    apriori_claim_count = np.sum(\n",
    "        (claim_count_chainladder.ultimate_['frequency'] * weights).iloc[:, :, -12:, :], \n",
    "        axis=2, keepdims=True\n",
    "    ) / np.sum(weights.iloc[:, :, -12:, :], axis=2, keepdims=True)\n",
    "\n",
    "    # Bornhuetter-Ferguson\n",
    "    claim_count_bf = cl.BornhuetterFerguson(\n",
    "        apriori=apriori_claim_count\n",
    "    ).fit(\n",
    "        triangle_combined[['claim_count','exposure']], \n",
    "        sample_weight=triangle_combined['exposure'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    # Cape Cod\n",
    "    claim_count_cc = cl.CapeCod().fit(\n",
    "        triangle_combined[['claim_count','exposure']],\n",
    "        sample_weight=triangle_combined['exposure'].latest_diagonal\n",
    "    )\n",
    "\n",
    "\n",
    "    # 2.2 Model Training - Net Incurred\n",
    "    # =================================\n",
    "\n",
    "    net_incurred_development_factors = cl.Development(n_periods=development_period_end).fit_transform(\n",
    "        triangle_combined[['net_claim_incurred','net_cost_per_policy','net_loss_ratio']]\n",
    "    )\n",
    "    for i in range(0, net_incurred_development_factors.ldf_.values.shape[0]):\n",
    "        net_incurred_development_factors.ldf_.values[i][:,0,development_period_end:] = 1\n",
    "        net_incurred_development_factors.cdf_.values[i][:,0,development_period_end:] = 1\n",
    "\n",
    "    # Chainladder\n",
    "    net_incurred_chainladder = cl.Chainladder().fit(net_incurred_development_factors)\n",
    "    \n",
    "    weights = triangle_combined['earnprem'].latest_diagonal\n",
    "    weights /= np.sum(weights, axis=2, keepdims=True)\n",
    "    apriori_net_incurred = np.sum(\n",
    "        (net_incurred_chainladder.ultimate_['net_loss_ratio'] * weights).iloc[:, :, -12:, :], \n",
    "        axis=2, keepdims=True\n",
    "    ) / np.sum(weights.iloc[:, :, -12:, :], axis=2, keepdims=True)\n",
    "\n",
    "    # Bornhuetter-Ferguson\n",
    "    net_incurred_bf = cl.BornhuetterFerguson(\n",
    "        apriori=apriori_net_incurred\n",
    "    ).fit(\n",
    "        net_incurred_development_factors, \n",
    "        sample_weight=triangle_combined['earnprem'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    # Cape Cod\n",
    "    net_incurred_cc = cl.CapeCod().fit(\n",
    "        net_incurred_development_factors,\n",
    "        sample_weight=triangle_combined['earnprem'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    # 3. Apply models to output granularity\n",
    "    # =================================\n",
    "\n",
    "    return apply_models_to_granularity(\n",
    "        input_data, valuation_date,\n",
    "        model_triangle_groups, output_triangle_groups,\n",
    "        claim_count_chainladder, claim_count_bf, claim_count_cc,\n",
    "        net_incurred_chainladder, net_incurred_bf, net_incurred_cc\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7a5358bd-d991-4a8a-8a4c-c1f40864309c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_model_results(claim_count_chainladder, claim_count_bf, claim_count_cc,\n",
    "                         net_incurred_chainladder, net_incurred_bf, net_incurred_cc,\n",
    "                         model_triangle_groups, debug=False):\n",
    "    \"\"\"Extract and format model results\"\"\"\n",
    "   \n",
    "    results = []\n",
    "   \n",
    "    # Claim count results\n",
    "    for model, suffix in [(claim_count_chainladder, '_chainladder'),\n",
    "                         (claim_count_bf, '_bf'),\n",
    "                         (claim_count_cc, '_cc')]:\n",
    "        df = model.ultimate_.to_frame().reset_index()\n",
    "       \n",
    "        if debug:\n",
    "            print(f\"\\nProcessing claim count model{suffix}\")\n",
    "            print(f\"Columns before processing: {df.columns.tolist()}\")\n",
    "            print(f\"DataFrame shape: {df.shape}\")\n",
    "       \n",
    "        # Handle the origin column rename\n",
    "        if 'origin' in df.columns:\n",
    "            df = df.rename(columns={'origin': 'acc_month'})\n",
    "       \n",
    "        # Rename claim_count column\n",
    "        if 'claim_count' in df.columns:\n",
    "            df = df.rename(columns={'claim_count': f'ultimate_claim_count{suffix}'})\n",
    "       \n",
    "        # Drop valuation column if it exists\n",
    "        if 'valuation' in df.columns:\n",
    "            df = df.drop(columns=['valuation'])\n",
    "       \n",
    "        if debug:\n",
    "            print(f\"Columns after processing: {df.columns.tolist()}\")\n",
    "           \n",
    "        results.append(df)\n",
    "   \n",
    "    # Net incurred results  \n",
    "    for model, suffix in [(net_incurred_chainladder, '_chainladder'),\n",
    "                         (net_incurred_bf, '_bf'),\n",
    "                         (net_incurred_cc, '_cc')]:\n",
    "        df = model.ultimate_.to_frame().reset_index()\n",
    "       \n",
    "        if debug:\n",
    "            print(f\"\\nProcessing net incurred model{suffix}\")\n",
    "            print(f\"Columns before processing: {df.columns.tolist()}\")\n",
    "            print(f\"DataFrame shape: {df.shape}\")\n",
    "       \n",
    "        # Handle the origin column rename\n",
    "        if 'origin' in df.columns:\n",
    "            df = df.rename(columns={'origin': 'acc_month'})\n",
    "       \n",
    "        # Drop valuation column if it exists\n",
    "        if 'valuation' in df.columns:\n",
    "            df = df.drop(columns=['valuation'])\n",
    "       \n",
    "        # Keep only the required columns\n",
    "        cols_to_keep = model_triangle_groups + ['acc_month']\n",
    "       \n",
    "        # Add net_claim_incurred if it exists, otherwise look for alternatives\n",
    "        if 'net_claim_incurred' in df.columns:\n",
    "            cols_to_keep.append('net_claim_incurred')\n",
    "            df = df[cols_to_keep].rename(columns={\n",
    "                'net_claim_incurred': f'ultimate_net_incurred{suffix}'\n",
    "            })\n",
    "        else:\n",
    "            # Handle case where column might have different name\n",
    "            incurred_cols = [col for col in df.columns if 'incurred' in col.lower()]\n",
    "            if incurred_cols:\n",
    "                cols_to_keep.append(incurred_cols[0])\n",
    "                df = df[cols_to_keep].rename(columns={\n",
    "                    incurred_cols[0]: f'ultimate_net_incurred{suffix}'\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: No incurred column found for {suffix}\")\n",
    "                print(f\"Available columns: {df.columns.tolist()}\")\n",
    "                continue\n",
    "       \n",
    "        if debug:\n",
    "            print(f\"Columns after processing: {df.columns.tolist()}\")\n",
    "            print(f\"Final columns to keep: {cols_to_keep}\")\n",
    "       \n",
    "        results.append(df)\n",
    "   \n",
    "    # Merge all results\n",
    "    if not results:\n",
    "        raise ValueError(\"No model results could be extracted\")\n",
    "   \n",
    "    final_result = results[0]\n",
    "    for i, df in enumerate(results[1:], 1):\n",
    "        if debug:\n",
    "            print(f\"\\nMerging result {i+1}\")\n",
    "            print(f\"Current result columns: {final_result.columns.tolist()}\")\n",
    "            print(f\"Merging with columns: {df.columns.tolist()}\")\n",
    "            print(f\"Merge keys: {model_triangle_groups + ['acc_month']}\")\n",
    "       \n",
    "        final_result = pd.merge(\n",
    "            final_result, df,\n",
    "            on=model_triangle_groups + ['acc_month'],\n",
    "            how='outer'\n",
    "        )\n",
    "   \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a3edd87a-8583-4c35-8802-fc837987694f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def apply_proportional_allocation(df, model_groups, output_groups):\n",
    "    \"\"\"\n",
    "    Apply proportional allocation when output granularity is finer than model granularity\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify the additional grouping variables\n",
    "    additional_groups = [col for col in output_groups if col not in model_groups]\n",
    "    \n",
    "    if not additional_groups:\n",
    "        return df\n",
    "    \n",
    "    # Calculate proportions for allocation\n",
    "    proportion_cols = ['exposure', 'earnprem', 'earnprem_indexed']\n",
    "    \n",
    "    for prop_col in proportion_cols:\n",
    "        if prop_col in df.columns:\n",
    "            # Calculate totals at model granularity\n",
    "            totals = df.groupby(model_groups + ['acc_month'])[prop_col].sum().reset_index()\n",
    "            totals.columns = model_groups + ['acc_month', f'{prop_col}_total']\n",
    "            \n",
    "            # Merge back to get proportions\n",
    "            df = pd.merge(df, totals, on=model_groups + ['acc_month'], how='left')\n",
    "            df[f'{prop_col}_proportion'] = df[prop_col] / df[f'{prop_col}_total']\n",
    "            df[f'{prop_col}_proportion'] = df[f'{prop_col}_proportion'].fillna(0)\n",
    "    \n",
    "    # Apply proportional allocation to ultimate values\n",
    "    ultimate_cols = [col for col in df.columns if col.startswith('ultimate_')]\n",
    "    \n",
    "    for col in ultimate_cols:\n",
    "        if 'claim_count' in col:\n",
    "            # Use exposure for claim count allocation\n",
    "            if 'exposure_proportion' in df.columns:\n",
    "                df[col] = df[col] * df['exposure_proportion']\n",
    "        else:\n",
    "            # Use earnprem for net incurred allocation\n",
    "            if 'earnprem_proportion' in df.columns:\n",
    "                df[col] = df[col] * df['earnprem_proportion']\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    temp_cols = [col for col in df.columns if col.endswith('_total') or col.endswith('_proportion')]\n",
    "    df = df.drop(columns=temp_cols)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "abaf33dc-f30c-4999-bd8f-b844d717e518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def apply_models_to_granularity(input_data, valuation_date,\n",
    "                               model_triangle_groups, output_triangle_groups,\n",
    "                               claim_count_chainladder, claim_count_bf, claim_count_cc,\n",
    "                               net_incurred_chainladder, net_incurred_bf, net_incurred_cc):\n",
    "    \"\"\"\n",
    "    Apply models built at one granularity to output at another granularity\n",
    "    \"\"\"\n",
    "   \n",
    "    # Get model results at model granularity\n",
    "    model_results = extract_model_results(\n",
    "        claim_count_chainladder, claim_count_bf, claim_count_cc,\n",
    "        net_incurred_chainladder, net_incurred_bf, net_incurred_cc,\n",
    "        model_triangle_groups, debug=False  # Set to False once working\n",
    "    )\n",
    "   \n",
    "    # Prepare base data at output granularity\n",
    "    data_hidden = input_data[\n",
    "        (input_data['obs_month'] <= valuation_date) &\n",
    "        (input_data['acc_month'] <= valuation_date)\n",
    "    ]\n",
    "   \n",
    "    output_base = data_hidden.groupby(\n",
    "        output_triangle_groups + ['acc_month'],\n",
    "        as_index=False\n",
    "    ).agg({\n",
    "        'exposure': 'sum',\n",
    "        'earnprem': 'sum',\n",
    "        'earnprem_indexed': 'sum',\n",
    "        'claim_count': 'sum',\n",
    "        'net_claim_incurred': 'sum',\n",
    "        'gross_claim_incurred': 'sum',\n",
    "        'recoveries': 'sum',\n",
    "        'net_claim_incurred_indexed': 'sum',\n",
    "        'gross_claim_incurred_indexed': 'sum',\n",
    "        'recoveries_indexed': 'sum',\n",
    "        'product_group': 'first'\n",
    "    }).rename(columns={\n",
    "        'claim_count': 'reported_to_date_claim_count',\n",
    "        'net_claim_incurred': 'reported_to_date_net_claim_incurred',\n",
    "        'gross_claim_incurred': 'reported_to_date_gross_claim_incurred',\n",
    "        'recoveries': 'reported_to_date_recoveries',\n",
    "        'net_claim_incurred_indexed': 'reported_to_date_net_claim_incurred_indexed',\n",
    "        'gross_claim_incurred_indexed': 'reported_to_date_gross_claim_incurred_indexed',\n",
    "        'recoveries_indexed': 'reported_to_date_recoveries_indexed'\n",
    "    })\n",
    "   \n",
    "    # Merge with model results\n",
    "    output_results = pd.merge(\n",
    "        output_base,\n",
    "        model_results,\n",
    "        on=model_triangle_groups + ['acc_month'],\n",
    "        how='left'\n",
    "    )\n",
    "   \n",
    "    # Apply proportional allocation if output is more granular\n",
    "    if set(output_triangle_groups) > set(model_triangle_groups):\n",
    "        output_results = apply_proportional_allocation(output_results, model_triangle_groups, output_triangle_groups)\n",
    "   \n",
    "    # Add full triangle results for comparison\n",
    "    data_full = input_data.copy()\n",
    "    full_results = get_full_triangle_results(data_full, output_triangle_groups)\n",
    "   \n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        full_results,\n",
    "        on=output_triangle_groups + ['acc_month'],\n",
    "        how='left'\n",
    "    )\n",
    "   \n",
    "    output_results['valuation_date'] = pd.to_datetime(valuation_date)\n",
    "   \n",
    "    return output_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "31d0af69-a00c-49c2-9799-5aa1835b38e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_full_triangle_results(data_full, triangle_groups):\n",
    "    \"\"\"Get actual results from full triangle for comparison\"\"\"\n",
    "    \n",
    "    triangle_full = cl.Triangle(\n",
    "        data_full,\n",
    "        origin='acc_month',\n",
    "        development='obs_month',\n",
    "        columns=[\n",
    "            'claim_count',\n",
    "            'net_claim_incurred',\n",
    "            'gross_claim_incurred',\n",
    "            'net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed',\n",
    "        ],\n",
    "        index=triangle_groups,\n",
    "        cumulative=False,\n",
    "    ).incr_to_cum()\n",
    "\n",
    "    return triangle_full.latest_diagonal.to_frame().reset_index().drop(\n",
    "        columns=['valuation']\n",
    "    ).rename(columns={\n",
    "        'origin': 'acc_month',\n",
    "        'claim_count': 'latest_view_claim_count',\n",
    "        'net_claim_incurred': 'latest_view_net_claim_incurred',\n",
    "        'gross_claim_incurred': 'latest_view_gross_claim_incurred',\n",
    "        'net_claim_incurred_indexed': 'latest_view_net_claim_incurred_indexed',\n",
    "        'gross_claim_incurred_indexed': 'latest_view_gross_claim_incurred_indexed',\n",
    "        'recoveries_indexed': 'latest_view_recoveries_indexed'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c43b3414-ecac-477d-a290-3074ffb5420d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(claim_data, exposure_data):\n",
    "    # Ensure date columns are datetime\n",
    "    claim_data['acc_month'] = pd.to_datetime(claim_data['acc_month'])\n",
    "    claim_data['obs_month'] = pd.to_datetime(claim_data['obs_month'])\n",
    "    exposure_data['acc_month'] = pd.to_datetime(exposure_data['acc_month'])\n",
    "    \n",
    "    # Determine merge keys based on available columns\n",
    "    claim_cols = set(claim_data.columns)\n",
    "    exposure_cols = set(exposure_data.columns)\n",
    "    \n",
    "    # Common columns for merging (excluding metrics)\n",
    "    merge_keys = ['acc_month', 'dev_month']\n",
    "    potential_keys = ['channel', 'claim_type', 'premcls', 'usage'] # Adjust for all possible column names\n",
    "    \n",
    "    for key in potential_keys:\n",
    "        if key in claim_cols and key in exposure_cols:\n",
    "            merge_keys.append(key)\n",
    "    \n",
    "    print(f\"Merging on keys: {merge_keys}\")\n",
    "    \n",
    "    # Merge claim and exposure data\n",
    "    combined_data = pd.merge(\n",
    "        claim_data,\n",
    "        exposure_data,\n",
    "        on=merge_keys,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing exposure values with 0 (or handle as appropriate)\n",
    "    combined_data['exposure'] = combined_data['exposure'].fillna(0)\n",
    "    combined_data['earnprem'] = combined_data['earnprem'].fillna(0)\n",
    "    \n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d6c759ec-b113-491d-a0f9-6c4eeeece6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parallel_runs(input_data, valuation_dates, development_period_end, model_triangle_groups=['channel', 'claim_type'], output_triangle_groups=['channel', 'claim_type', 'premcls']):\n",
    "    # Run in parallel\n",
    "    valuation_data = Parallel(n_jobs=-1)(delayed(\n",
    "        build_models_new\n",
    "    )(\n",
    "        input_data,\n",
    "        date,\n",
    "        development_period_end,\n",
    "        model_triangle_groups, # Build models at this level\n",
    "        output_triangle_groups  # Apply at this level\n",
    "    ) for date in tqdm(valuation_dates, desc=\"Processing valuation dates\"))\n",
    "\n",
    "    # Combine results into a single DataFrame\n",
    "    result = pd.concat(valuation_data, ignore_index=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "    combined_df = parallel_runs(input_data=curr_indexed_model_data\n",
    "                                , valuation_dates=valuation_dates\n",
    "                                , development_period_end=development_term\n",
    "                                , model_triangle_groups=['channel', 'claim_type']\n",
    "                                , output_triangle_groups=['channel', 'claim_type', 'premcls']) # parameterise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a1e41d30-d312-4d69-8ffe-e9d1eb8a4071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def indexation(data, cpi, indexation_date, join_column, indexation_columns, suffix='_indexed', unindex=False):\n",
    "    \"\"\"\n",
    "    Apply indexation (e.g., CPI adjustment) to specified columns in a dataset based on a reference date.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Input DataFrame containing the data to be indexed.\n",
    "    - indexation_date (str or pd.Timestamp): Date used as the reference for indexation (e.g., valuation date).\n",
    "    - join_column (str): Column in `data` representing time (e.g., transaction date) to merge with CPI data.\n",
    "    - indexation_columns (list): List of column names in `data` to apply indexation to.\n",
    "    - suffix (str, optional): Suffix to append to indexed column names. Defaults to '_indexed'.\n",
    "    - unindex (bool, optional): If True, reverses indexation (divides by factor); if False, applies it (multiplies).\n",
    "                                Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with new indexed columns and temporary columns removed.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If columns with the `suffix` already exist in `data`.\n",
    "    - Warning: If `unindex=True` and `suffix='_indexed'` (to avoid naming confusion).\n",
    "    \"\"\"\n",
    "\n",
    "    # Create new column names by appending the suffix to the original column names\n",
    "    indexation_columns1 = [col + suffix for col in indexation_columns]\n",
    "    \n",
    "    # Check if any of the new column names already exist in the DataFrame\n",
    "    for col in indexation_columns1:\n",
    "        if col in data.columns:\n",
    "            raise ValueError(f\"Column {col} already exists in data.\")\n",
    "\n",
    "    # Create a copy of the input DataFrame to avoid modifying the original\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Add a column for the indexation reference date (converted to datetime)\n",
    "    data['cpi_valuation_quarter'] = pd.to_datetime(indexation_date)\n",
    "\n",
    "    # Merge the data with CPI data based on the join_column (e.g., transaction date)\n",
    "    # Uses merge_asof to match to the nearest CPI quarter\n",
    "    data = pd.merge_asof(\n",
    "        data.sort_values(join_column),              # Sort data by join_column (e.g., transaction date)\n",
    "        cpi.sort_values('quarter'),                 # Sort CPI data by quarter (assumes 'cpi' is a global DataFrame)\n",
    "        left_on=join_column,                        # Column in `data` to match\n",
    "        right_on='quarter',                         # Column in `cpi` to match\n",
    "        direction='nearest'                         # Match to the nearest quarter\n",
    "    ).drop(columns=['quarter']).rename(columns={'cpi': 'cpi_txn'})            # Remove the redundant 'quarter' column from CPI, # Rename CPI column to indicate transaction CPI\n",
    "\n",
    "    # Merge again to get CPI for the indexation_date (valuation date)\n",
    "    data = pd.merge_asof(\n",
    "        data.sort_values('cpi_valuation_quarter'),  # Sort by the valuation date column\n",
    "        cpi.sort_values('quarter'),                 # Sort CPI data by quarter\n",
    "        left_on='cpi_valuation_quarter',            # Match on valuation date\n",
    "        right_on='quarter',                         # Match on CPI quarter\n",
    "        direction='nearest'                         # Match to the nearest quarter\n",
    "    ).drop(columns=['quarter']).rename(columns={'cpi': 'cpi_valuation'})  # Remove the redundant 'quarter' column  # Rename CPI column to indicate valuation CPI\n",
    "\n",
    "    # Calculate the indexation factor based on whether we're indexing or unindexing\n",
    "    if unindex:\n",
    "        # If unindexing, divide transaction CPI by valuation CPI (reverse adjustment)\n",
    "        data['indexation_factor'] = data['cpi_txn'] / data['cpi_valuation']\n",
    "        # Warn if the default suffix '_indexed' is used with unindexing\n",
    "        if suffix == '_indexed':\n",
    "            raise Warning(\"Unindexing is enabled. Please change the suffix from '_indexed' to avoid confusion\")\n",
    "    else:\n",
    "        # If indexing, divide valuation CPI by transaction CPI (standard adjustment)\n",
    "        data['indexation_factor'] = data['cpi_valuation'] / data['cpi_txn']\n",
    "\n",
    "    # Drop temporary columns used for calculation\n",
    "    data.drop(columns=['cpi_txn', 'cpi_valuation', 'cpi_valuation_quarter'], inplace=True)\n",
    "\n",
    "    # Apply the indexation factor to the specified columns and create new columns with the suffix\n",
    "    data[indexation_columns1] = data[indexation_columns].multiply(data['indexation_factor'], axis=0)\n",
    "    \n",
    "    # Remove the indexation_factor column as it's no longer needed\n",
    "    data.drop(columns=['indexation_factor'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aefac494-0a44-44a9-a88e-1169f380c603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reshape_forecast_output(\n",
    "    df: pd.DataFrame,\n",
    "    response_prefixes: dict = {\n",
    "        'ultimate_claim_count': {\n",
    "            'response_type': 'count',\n",
    "            'actual_col': 'latest_view_claim_count',\n",
    "            'reported_col': 'reported_to_date_claim_count'\n",
    "        },\n",
    "        'ultimate_net_incurred': {\n",
    "            'response_type': 'net_incurred',\n",
    "            'actual_col': 'latest_view_net_claim_incurred',\n",
    "            'reported_col': 'reported_to_date_net_claim_incurred'\n",
    "        }\n",
    "    },\n",
    "    id_columns: list = ['acc_month', 'valuation_date', 'channel', 'claim_type']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reshapes ultimate model output DataFrame into long format with actual and predicted values.\n",
    "    Handles both claim count and net incurred amounts.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame.\n",
    "    - response_prefixes: Dictionary mapping response prefixes to their metadata including:\n",
    "        - response_type: Label for type of response ('count', 'net_incurred', etc.)\n",
    "        - actual_col: Column name representing actual value\n",
    "        - reported_col: Column name representing reported-to-date value\n",
    "    - id_columns: List of identifying columns to retain (e.g. ['acc_month', 'valuation_date', ...]).\n",
    "\n",
    "    Returns:\n",
    "    - A tidy DataFrame with columns: id_columns + ['model', 'actual', 'predicted', 'latest_view_*', 'reported_to_date_*', 'response']\n",
    "    \"\"\"\n",
    "    result_dfs = []\n",
    "\n",
    "    # Process each response prefix type\n",
    "    for response_prefix, config in response_prefixes.items():\n",
    "        response_type = config['response_type']\n",
    "        actual_col = config['actual_col']\n",
    "        reported_col = config['reported_col']\n",
    "        \n",
    "        # Skip if required columns are not in the DataFrame\n",
    "        if actual_col not in df.columns or reported_col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Identify ultimate columns for the given response prefix\n",
    "        ultimate_cols = [col for col in df.columns if col.startswith(response_prefix)]\n",
    "        \n",
    "        if not ultimate_cols:\n",
    "            continue\n",
    "            \n",
    "        models = [col.split('_')[-1] for col in ultimate_cols]\n",
    "\n",
    "        for model, col_name in zip(models, ultimate_cols):\n",
    "            # Make sure all required columns exist before proceeding\n",
    "            required_cols = id_columns + [col_name, actual_col, reported_col]\n",
    "            if not all(col in df.columns for col in required_cols):\n",
    "                continue\n",
    "                \n",
    "            model_df = df[required_cols].copy()\n",
    "\n",
    "            model_df['model'] = model\n",
    "            model_df['actual'] = model_df[actual_col] - model_df[reported_col]\n",
    "            model_df['predicted'] = model_df[col_name] - model_df[reported_col]\n",
    "            model_df['response'] = response_type\n",
    "\n",
    "            # Rename columns to standardised names to ensure they're consistent\n",
    "            model_df = model_df.rename(columns={\n",
    "                actual_col: 'latest_view',\n",
    "                reported_col: 'reported_to_date'\n",
    "            })\n",
    "\n",
    "            # Drop the original ultimate column\n",
    "            model_df = model_df.drop(columns=[col_name])\n",
    "            result_dfs.append(model_df)\n",
    "\n",
    "    # If no results were found, return empty DataFrame with correct columns\n",
    "    if not result_dfs:\n",
    "        return pd.DataFrame(columns=id_columns + ['model', 'actual', 'predicted', 'latest_view', 'reported_to_date', 'response'])\n",
    "    \n",
    "    # Combine all results\n",
    "    result = pd.concat(result_dfs, ignore_index=True)\n",
    "    \n",
    "    # Ensure consistent column order\n",
    "    result = result[id_columns + ['model', 'actual', 'predicted', 'latest_view', 'reported_to_date', 'response']]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "00ed762f-07c8-4be0-acef-a7b34ee8bbcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mae_excl_last_6_months(data, actual_col, pred_cols, groupby_cols, date_col='valuation_date'):\n",
    "    results = []\n",
    "\n",
    "    for keys, group in data.groupby(groupby_cols):\n",
    "        # Ensure date is datetime\n",
    "        group[date_col] = pd.to_datetime(group[date_col])\n",
    "\n",
    "        # Aggregate actual and predicted per valuation_date\n",
    "        agg_list = {'valuation_date': group[date_col].unique()}\n",
    "        agg_df = pd.DataFrame({'valuation_date': group[date_col]})\n",
    "        agg_df[actual_col] = group[actual_col]\n",
    "        for col in pred_cols:\n",
    "            agg_df[col] = group[col]\n",
    "\n",
    "        agg_df = agg_df.groupby('valuation_date').sum().reset_index()\n",
    "\n",
    "        # Define cutoff date\n",
    "        cutoff_date = agg_df['valuation_date'].max() - pd.DateOffset(months=6)\n",
    "        filtered_df = agg_df[agg_df['valuation_date'] <= cutoff_date]\n",
    "\n",
    "        # Compute MAE per model\n",
    "        mae_dict = {}\n",
    "        for col in pred_cols:\n",
    "            mae = np.abs(filtered_df[actual_col] - filtered_df[col]).mean()\n",
    "            mae_dict[col] = mae\n",
    "\n",
    "        if not mae_dict:\n",
    "            continue  # Skip if no models were evaluated\n",
    "\n",
    "        # Find best model\n",
    "        best_model = min(mae_dict, key=mae_dict.get)\n",
    "\n",
    "        # Save results\n",
    "        results.append({\n",
    "            **dict(zip(groupby_cols, keys if isinstance(keys, tuple) else [keys])),\n",
    "            **{f'mae_{col}': val for col, val in mae_dict.items()},\n",
    "            'best_model': best_model\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "88c459b8-8645-4578-946a-db1028bce4f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_best_method(input_df, metric, groupby_cols):\n",
    "\n",
    "    if metric == 'claim_count':\n",
    "        actual_col = 'latest_view_claim_count'\n",
    "    else:\n",
    "        actual_col = 'latest_view_net_claim_incurred'\n",
    "\n",
    "    best_method = calculate_mae_excl_last_6_months(\n",
    "        input_df,\n",
    "        actual_col=actual_col,\n",
    "        pred_cols=[\n",
    "            'ultimate_' + metric + '_chainladder',\n",
    "            'ultimate_' + metric + '_bf',\n",
    "            'ultimate_' + metric + '_cc'\n",
    "        ],\n",
    "        groupby_cols=groupby_cols,\n",
    "        date_col='valuation_date'\n",
    "    )\n",
    "\n",
    "    if metric == 'claim_count':\n",
    "        best_method['response'] = 'count'\n",
    "    else:\n",
    "        best_method['response'] = 'net_incurred'\n",
    "\n",
    "    best_method['model'] = best_method['best_model'].str.split(\"_\").str[-1]\n",
    "\n",
    "    return best_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5664e04b-9bc0-4721-90a2-4952d68657a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i, product in enumerate(product_configs, start=1):\n",
    "    print(f\"\\n=== [{i}/{len(product_configs)}] Processing Product: {product} ===\")\n",
    "\n",
    "    # 1. Setup\n",
    "    # =================================\n",
    "    print(\"     [1/8] Retrieving Configurations\")\n",
    "    curr_product_short = product_configs[product][\"product_short\"]\n",
    "    curr_claim_data = product_configs[product][\"claim_data\"]\n",
    "    curr_expo_data = product_configs[product][\"expo_data\"]\n",
    "    curr_model_data = product_configs[product][\"model_data\"]\n",
    "    # curr_aggLevels = [product_configs[product][\"main_level\"]] + product_configs[product][\"sub_levels\"]\n",
    "    curr_aggLevels = ['channel', 'claim_type', 'premcls']\n",
    "\n",
    "    # 2. Indexation\n",
    "    # =================================\n",
    "    print(\"     [2/8] Column Indexation\")\n",
    "    combined_data_adequacy = prepare_data(claim_data_for_adequacy_analysis, expo_data)\n",
    "    cols_to_index = ['earnprem', 'net_claim_incurred', 'gross_claim_incurred', 'recoveries']\n",
    "    temp_df1 = combined_data_adequacy.copy()\n",
    "    temp_df1[cols_to_index] = temp_df1[cols_to_index].astype(float)\n",
    "    \n",
    "    # Read cpi file and format dataframe\n",
    "    cpi_df = pd.read_csv(cpi_file_path)\n",
    "    cpi_df['quarter'] = pd.to_datetime(cpi_df['quarter'], format=\"%d/%m/%Y\")\n",
    "\n",
    "    curr_indexed_model_data = indexation(temp_df1, cpi=cpi_df, indexation_date=last_day_previous_month, join_column='obs_month', indexation_columns=cols_to_index) \n",
    "\n",
    "    # 3. Modelling\n",
    "    # =================================\n",
    "    print(\"     [3/8] Modelling\")\n",
    "    combined_df = parallel_runs(input_data=curr_indexed_model_data\n",
    "                                , valuation_dates=valuation_dates\n",
    "                                , development_period_end=development_term\n",
    "                                , model_triangle_groups=['channel', 'claim_type']\n",
    "                                , output_triangle_groups=['channel', 'claim_type', 'premcls']) # parameterise\n",
    "\n",
    "    # 4. Reshape Output\n",
    "    # =================================\n",
    "    print(\"     [4/8] Reshaping Output\")\n",
    "    result = reshape_forecast_output(\n",
    "        df=combined_df,\n",
    "        response_prefixes={\n",
    "            'ultimate_claim_count': {\n",
    "                'response_type': 'count',\n",
    "                'actual_col': 'latest_view_claim_count',\n",
    "                'reported_col': 'reported_to_date_claim_count'\n",
    "            },\n",
    "            'ultimate_net_incurred': {\n",
    "                'response_type': 'net_incurred',\n",
    "                'actual_col': 'latest_view_net_claim_incurred',\n",
    "                'reported_col': 'reported_to_date_net_claim_incurred'\n",
    "            }\n",
    "        },\n",
    "        id_columns=['acc_month', 'valuation_date'] + curr_aggLevels\n",
    "    )\n",
    "\n",
    "    # 5. Best Method Selection\n",
    "    # =================================\n",
    "    print(\"     [5/8] Best Method Selection\")\n",
    "    claim_count_best_method = get_best_method(combined_df, 'claim_count', curr_aggLevels)\n",
    "    net_incurred_best_method = get_best_method(combined_df, 'net_incurred', curr_aggLevels)\n",
    "\n",
    "    # Union best method dataframes to be used for subsetting\n",
    "    best_method_union = pd.concat([claim_count_best_method, net_incurred_best_method]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Get best model results\n",
    "    bm_results = result.merge(\n",
    "        best_method_union[curr_aggLevels + ['model', 'response']],\n",
    "        on=curr_aggLevels + ['model', 'response'],\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # Retrieve latest valuation date for selected models\n",
    "\n",
    "    bm_results = bm_results[bm_results['valuation_date'] == bm_results['valuation_date'].max()]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f6af0e-c7cc-4220-9f03-972b673212ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee2f2ec-3a64-43a7-ae5c-e512f23d1e6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Transformations\n",
    "# =================================\n",
    "print(\"     [6/8] Transformations\")\n",
    "# Consolidation\n",
    "ultimates_pre = pd.merge(\n",
    "    curr_claim_data.groupby(['acc_month'] + curr_aggLevels)[['claim_count', 'net_claim_incurred']].sum().reset_index(),\n",
    "    bm_results,\n",
    "    on=['acc_month'] + curr_aggLevels,\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Merge with expo_data\n",
    "ultimates_pre = pd.merge(ultimates_pre, curr_expo_data, on=['acc_month', 'channel', 'premcls']) # - TEMPORARY, USE CHANNEL ONLY, expo data should match granularity of aggregation\n",
    "\n",
    "# Fill missing values and convert to int64 for relevant columns\n",
    "columns_to_fix = ['net_claim_incurred', 'claim_count', 'predicted',  'exposure']\n",
    "ultimates_pre[columns_to_fix] = ultimates_pre[columns_to_fix].fillna(0).astype('int64')\n",
    "ultimates_pre['product'] = product\n",
    "\n",
    "# Pivoting\n",
    "pivot_indexes = ['acc_month', 'product', 'claim_count', 'net_claim_incurred', 'earnprem', 'exposure'] + curr_aggLevels\n",
    "\n",
    "# Pivoting predicted values\n",
    "predicted_pivot = ultimates_pre.pivot_table(index=pivot_indexes, \n",
    "                                columns='response', \n",
    "                                values='predicted').reset_index()\n",
    "predicted_pivot.columns.name = None\n",
    "predicted_pivot = predicted_pivot.rename(columns={\n",
    "    'count': 'ibnr_count',\n",
    "    'net_incurred': 'ibnr_incurred'\n",
    "})\n",
    "\n",
    "# Pivoting model values\n",
    "model_pivot = ultimates_pre.pivot_table(index=pivot_indexes, \n",
    "                            columns='response', \n",
    "                            values='model', aggfunc='first').reset_index()\n",
    "model_pivot.columns.name = None\n",
    "model_pivot = model_pivot.rename(columns={\n",
    "    'count': 'count_model',\n",
    "    'net_incurred': 'incurred_model'\n",
    "})\n",
    "\n",
    "# Merge the two pivoted DataFrames\n",
    "ultimates_df = pd.merge(predicted_pivot, model_pivot, on=pivot_indexes)\n",
    "\n",
    "# Create ultimate count and ultimate incurred columns\n",
    "ultimates_df['ultimate_count'] = ultimates_df['claim_count'] + ultimates_df['ibnr_count']\n",
    "ultimates_df['ultimate_incurred'] = ultimates_df['net_claim_incurred'] + ultimates_df['ibnr_incurred']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8bc38ab-71de-4066-9634-ec12b158cae5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752799724618}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ultimates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51509fe6-bff8-4bb2-a6ea-af33c056c7c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 7. Inflation Adjustment\n",
    "# =================================\n",
    "print(\"     [7/8] Inflation Adjustment\")\n",
    "temp_df2 = ultimates_df.copy()\n",
    "\n",
    "# Add Quarter to Ultimates Table\n",
    "temp_df2['quarter'] = temp_df2['acc_month'].dt.to_period('Q').astype(str).str.replace('Q', 'Q', regex=False)\n",
    "\n",
    "# CPI\n",
    "ultimates_with_cpi = temp_df2.merge(cpi_by_quarter[['quarter', 'cpi']], on='quarter', how='left')\n",
    "\n",
    "# Base CPI\n",
    "base_cpi_quarter = sorted(ultimates_with_cpi['quarter'].unique())[-2] # Get second-last quarter in dataset\n",
    "base_cpi_value = cpi_by_quarter.loc[cpi_by_quarter['quarter'] == base_cpi_quarter, 'cpi'].values[0]\n",
    "ultimates_with_cpi['base_cpi'] = base_cpi_value\n",
    "\n",
    "# Index Multiplier\n",
    "ultimates_with_cpi['index_multiplier'] = ultimates_with_cpi['base_cpi'] / ultimates_with_cpi['cpi']\n",
    "\n",
    "# Adjusted Incurreds\n",
    "ultimates_with_cpi['adj_ultimate_incurred'] = ultimates_with_cpi['ultimate_incurred'] * ultimates_with_cpi['index_multiplier']\n",
    "ultimates_with_cpi['adj_net_claim_incurred'] = ultimates_with_cpi['net_claim_incurred'] * ultimates_with_cpi['index_multiplier']\n",
    "\n",
    "    # # 8. Output to Table\n",
    "    # # =================================\n",
    "    # print(\"     [8/8] Writing to databricks table\")\n",
    "    # spark.createDataFrame(ultimates_with_cpi) \\\n",
    "    #     .write \\\n",
    "    #     .format(\"delta\") \\\n",
    "    #     .mode(\"overwrite\") \\\n",
    "    #     .option(\"overwriteSchema\", \"true\") \\\n",
    "    #     .saveAsTable(\"actuaries_prd.general.\" + curr_product_short.lower() + \"_ultimates_new\")\n",
    "\n",
    "    # print(f\"         - Saved to: actuaries_prd.general.{curr_product_short.lower()}_ultimates_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc198c83-f871-4a2c-99aa-d4a66c477908",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752735787093}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ultimates_with_cpi)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8049510483069592,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Bill) IBNR Configuration - (HH)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
