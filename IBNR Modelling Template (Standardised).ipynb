{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e117841-06f2-44de-a88c-89d6bdff2362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Modelling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76610c49-3bf6-4418-8a08-84ef0695c2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.1 - Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d03eef5-00a3-4126-976a-6f94cc02918f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_models(input_data, valuation_date, development_period_end = 24, triangle_groups = ['channel', 'claim_type'], n_periods = 12):\n",
    "    # Suppress all warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # 1. Data Processing\n",
    "    # =================================\n",
    "\n",
    "    data_hidden = input_data[\n",
    "        (input_data['obs_month'] <= valuation_date) &\n",
    "        (input_data['acc_month'] <= valuation_date)\n",
    "    ] \n",
    "\n",
    "    data_full = input_data.copy()\n",
    "\n",
    "    # Create triangle on partial data\n",
    "    triangle_combined = cl.Triangle(\n",
    "        data_hidden,\n",
    "        origin=\"acc_month\",\n",
    "        development=\"obs_month\",\n",
    "        columns=[\n",
    "            'claim_count',\n",
    "            'net_claim_incurred',\n",
    "            'gross_claim_incurred',\n",
    "            'net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed',\n",
    "            'earnprem_indexed',\n",
    "            'exposure',\n",
    "            'recoveries',\n",
    "            'earnprem'\n",
    "        ],\n",
    "        index=triangle_groups,\n",
    "        cumulative=False\n",
    "    ).incr_to_cum()\n",
    "\n",
    "    # Transformations\n",
    "    triangle_combined['frequency'] = triangle_combined['claim_count'] / triangle_combined['exposure']\n",
    "    triangle_combined['gross_cost_per_policy'] = triangle_combined['gross_claim_incurred'] / triangle_combined['exposure']\n",
    "    triangle_combined['gross_cost_per_policy_indexed'] = triangle_combined['gross_claim_incurred_indexed'] / triangle_combined['exposure']\n",
    "    triangle_combined['net_cost_per_policy'] = triangle_combined['net_claim_incurred'] / triangle_combined['exposure']\n",
    "    triangle_combined['net_cost_per_policy_indexed'] = triangle_combined['net_claim_incurred_indexed'] / triangle_combined['exposure']\n",
    "    triangle_combined['net_loss_ratio'] = triangle_combined['net_claim_incurred'] / triangle_combined['earnprem']\n",
    "    triangle_combined['net_loss_ratio_indexed'] = triangle_combined['net_claim_incurred_indexed'] / triangle_combined['earnprem_indexed']\n",
    "\n",
    "    # ------------------------------------\n",
    "    # FULL TRIANGLE (FOR ACTUAL RESULTS)\n",
    "    # ------------------------------------\n",
    "    triangle_combined_full = cl.Triangle(\n",
    "        data_full,\n",
    "        origin='acc_month',\n",
    "        development='obs_month',\n",
    "        columns=[\n",
    "            'claim_count',\n",
    "            'net_claim_incurred',\n",
    "            'gross_claim_incurred',\n",
    "            'net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed',\n",
    "            'earnprem_indexed',\n",
    "            'exposure',\n",
    "            'recoveries',\n",
    "            'earnprem'\n",
    "        ],\n",
    "        index=triangle_groups,\n",
    "        cumulative=False,\n",
    "    ).incr_to_cum()\n",
    "\n",
    "    output_actual_results = triangle_combined_full[\n",
    "        [\n",
    "            \"net_claim_incurred\",\n",
    "            \"gross_claim_incurred\",\n",
    "            \"claim_count\",\n",
    "            \"net_claim_incurred_indexed\",\n",
    "            \"gross_claim_incurred_indexed\",\n",
    "            \"recoveries_indexed\",\n",
    "        ]\n",
    "    ].latest_diagonal.to_frame().reset_index().drop(\n",
    "        columns=['valuation'] # Drop valuation and manually append after\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'origin' : 'acc_month',\n",
    "            'claim_count': 'latest_view_claim_count',\n",
    "            'net_claim_incurred': 'latest_view_net_claim_incurred',\n",
    "            'gross_claim_incurred': 'latest_view_gross_claim_incurred',\n",
    "            'recoveries': 'latest_view_recoveries',\n",
    "            'net_claim_incurred_indexed': 'latest_view_net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed': 'latest_view_gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed': 'latest_view_recoveries_indexed'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 2.1 Model Training - Claim Count\n",
    "    # =================================\n",
    "\n",
    "    # Build initial development triangle and replace the development factors to assume fully developed in {development_period_end} periods\n",
    "    claim_count_development_factors  = cl.Development(n_periods=n_periods).fit_transform(triangle_combined[['claim_count','frequency']])\n",
    "    for i in range(0, claim_count_development_factors.ldf_.values.shape[0]):\n",
    "        claim_count_development_factors.ldf_.values[i][:,0,development_period_end:] = 1\n",
    "        claim_count_development_factors.cdf_.values[i][:,0,development_period_end:] = 1\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 1 - Chainladder\n",
    "    # ------------------------------------\n",
    "    claim_count_chainladder = cl.Chainladder().fit(claim_count_development_factors)\n",
    "\n",
    "    weights = triangle_combined['exposure'].latest_diagonal\n",
    "    weights /= np.sum(weights, axis=2, keepdims=True)\n",
    "\n",
    "    apriori_claim_count = np.sum((claim_count_chainladder.ultimate_['frequency'] * weights).iloc[:, :, -12:, :], axis=2, keepdims=True) / np.sum(weights.iloc[:, :, -12:, :], axis=2, keepdims=True)\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 2 - Bornhuetter-Ferguson\n",
    "    # ------------------------------------\n",
    "    claim_count_bf = cl.BornhuetterFerguson(\n",
    "        apriori=  apriori_claim_count\n",
    "    ).fit(\n",
    "        triangle_combined[['claim_count','exposure']]\n",
    "        , sample_weight = triangle_combined['exposure'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 3 - Cape Cod\n",
    "    # ------------------------------------\n",
    "    claim_count_cc = cl.CapeCod().fit(\n",
    "        triangle_combined[['claim_count','exposure']],\n",
    "        sample_weight=triangle_combined['exposure'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    output_triangle = triangle_combined[\n",
    "        [\n",
    "            \"exposure\",\n",
    "            \"earnprem\",\n",
    "            \"earnprem_indexed\",\n",
    "            \"claim_count\",\n",
    "            \"net_claim_incurred\",\n",
    "            \"gross_claim_incurred\",\n",
    "            \"recoveries\",\n",
    "            \"net_claim_incurred_indexed\",\n",
    "            \"gross_claim_incurred_indexed\",\n",
    "            \"recoveries_indexed\",\n",
    "        ]\n",
    "    ].latest_diagonal.to_frame().reset_index().drop(\n",
    "        columns=['valuation']\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'origin' : 'acc_month',\n",
    "            'claim_count': 'reported_to_date_claim_count',\n",
    "            'net_claim_incurred': 'reported_to_date_net_claim_incurred',\n",
    "            'gross_claim_incurred': 'reported_to_date_gross_claim_incurred',\n",
    "            'recoveries': 'reported_to_date_recoveries',\n",
    "            'net_claim_incurred_indexed': 'reported_to_date_net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed': 'reported_to_date_gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed': 'reported_to_date_recoveries_indexed'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 2.2 Model Training - Net Incurred\n",
    "    # =================================\n",
    "\n",
    "    # Build initial development triangle and replace the development factors to assume fully developed in {development_period_end} periods\n",
    "    net_incurred_development_factors  = cl.Development(n_periods=n_periods).fit_transform(triangle_combined[['net_claim_incurred','net_cost_per_policy','net_loss_ratio']])\n",
    "    for i in range(0, net_incurred_development_factors.ldf_.values.shape[0]):\n",
    "        net_incurred_development_factors.ldf_.values[i][:,0,development_period_end:] = 1\n",
    "        net_incurred_development_factors.cdf_.values[i][:,0,development_period_end:] = 1\n",
    "\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 1 - Chainladder\n",
    "    # ------------------------------------\n",
    "    \n",
    "    # Fit a chainladder model using the adjusted development factors from dev object\n",
    "    net_incurred_chainladder = cl.Chainladder().fit(net_incurred_development_factors)\n",
    "\n",
    "    weights = triangle_combined['earnprem'].latest_diagonal\n",
    "    weights /= np.sum(weights, axis=2, keepdims=True)\n",
    "\n",
    "    apriori_net_incurred = np.sum((net_incurred_chainladder.ultimate_['net_loss_ratio'] * weights).iloc[:, :, -12:, :], axis=2, keepdims=True) / np.sum(weights.iloc[:, :, -12:, :], axis=2, keepdims=True)\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 2 - Bornhuetter-Ferguson\n",
    "    # ------------------------------------\n",
    "\n",
    "    net_incurred_bf = cl.BornhuetterFerguson(\n",
    "        apriori=apriori_net_incurred\n",
    "    ).fit(\n",
    "        net_incurred_development_factors, \n",
    "        sample_weight=triangle_combined['earnprem'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 3 - Cape Cod\n",
    "    # ------------------------------------\n",
    "\n",
    "    net_incurred_cc = cl.CapeCod().fit(\n",
    "        net_incurred_development_factors,  # Use the same dev factors!\n",
    "        sample_weight=triangle_combined['earnprem'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    # 3. Joins to Final Dataframe\n",
    "    # =================================\n",
    "\n",
    "    # ------------------------------------\n",
    "    # FULL TRIANGLE - ACTUAL RESULTS\n",
    "    # ------------------------------------\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_triangle,\n",
    "        output_actual_results,\n",
    "        left_on  = triangle_groups + ['acc_month'],\n",
    "        right_on =  triangle_groups + ['acc_month'],\n",
    "        how='left'\n",
    "    ).drop(\n",
    "        columns=['exposure']\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # COUNT - CHAIN LADDER\n",
    "    # ------------------------------------\n",
    "    output_claim_count_chainladder = claim_count_chainladder.ultimate_.to_frame().reset_index(\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'claim_count': 'ultimate_claim_count_chainladder'\n",
    "            ,'origin' : 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_claim_count_chainladder,\n",
    "        left_on  = triangle_groups + ['acc_month'],\n",
    "        right_on =  triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # NET INCURRED - CHAIN LADDER\n",
    "    # ------------------------------------\n",
    "\n",
    "    output_net_incurred_chainladder = net_incurred_chainladder.ultimate_.to_frame().reset_index().drop(\n",
    "        columns=['valuation', 'net_cost_per_policy']\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'net_claim_incurred': 'ultimate_net_incurred_chainladder',\n",
    "            'origin': 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_net_incurred_chainladder,\n",
    "        left_on = triangle_groups + ['acc_month'],\n",
    "        right_on = triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # COUNT - BORNHUETTER-FERGUSON\n",
    "    # ------------------------------------\n",
    "    output_claim_count_bf = claim_count_bf.ultimate_.to_frame().reset_index(\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'claim_count': 'ultimate_claim_count_bf'\n",
    "            ,'origin' : 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_claim_count_bf,\n",
    "        left_on  = triangle_groups + ['acc_month'],\n",
    "        right_on =  triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # NET INCURRED - BORNHUETTER-FERGUSON\n",
    "    # ------------------------------------\n",
    "\n",
    "    output_net_incurred_bf = net_incurred_bf.ultimate_.to_frame().reset_index().drop(\n",
    "        columns=['valuation', 'net_cost_per_policy', 'net_loss_ratio']\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'net_claim_incurred': 'ultimate_net_incurred_bf',\n",
    "            'origin': 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_net_incurred_bf,\n",
    "        left_on = triangle_groups + ['acc_month'],\n",
    "        right_on = triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # COUNT - CAPE COD\n",
    "    # ------------------------------------\n",
    "    output_claim_count_cc = claim_count_cc.ultimate_.to_frame().reset_index().rename(\n",
    "        columns={\n",
    "            'claim_count': 'ultimate_claim_count_cc',\n",
    "            'origin': 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_claim_count_cc,\n",
    "        left_on=triangle_groups + ['acc_month'],\n",
    "        right_on=triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # NET INCURRED - CAPE COD\n",
    "    # ------------------------------------\n",
    "\n",
    "    output_net_incurred_cc = net_incurred_cc.ultimate_.to_frame().reset_index().drop(\n",
    "        columns=['valuation', 'net_cost_per_policy', 'net_loss_ratio']\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'net_claim_incurred': 'ultimate_net_incurred_cc',\n",
    "            'origin': 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_net_incurred_cc,\n",
    "        left_on=triangle_groups + ['acc_month'],\n",
    "        right_on=triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    # Final Output\n",
    "    output_results['valuation_date'] = pd.to_datetime(valuation_date)\n",
    "\n",
    "    # Aggregate required columns from the original data\n",
    "    additional_fields = input_data.groupby(triangle_groups + ['acc_month'], as_index=False).agg({\n",
    "        'product_group': 'first'  # Assuming 'product' is constant within each group\n",
    "    })\n",
    "\n",
    "    # Merge into the final output\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        additional_fields,\n",
    "        on=triangle_groups + ['acc_month'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    return output_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba86bb0b-f0e5-4cdd-bd07-bcb78e7aacf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2 - Parallel Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420777bd-1866-42cc-befd-949764da5999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parallel_runs(input_data, triangle_groups, valuation_dates):\n",
    "    # Run in parallel\n",
    "    valuation_data = Parallel(n_jobs=-1)(delayed(\n",
    "        build_models\n",
    "    )(\n",
    "        input_data,\n",
    "        date,\n",
    "        development_period_end,\n",
    "        triangle_groups,\n",
    "        n_periods\n",
    "    ) for date in tqdm(valuation_dates, desc=\"Processing valuation dates\"))\n",
    "\n",
    "    # Combine results into a single DataFrame\n",
    "    result = pd.concat(valuation_data, ignore_index=True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3979ece-94f1-4c0b-a1dd-bfc075daa156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Transformation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790c6bea-f750-4535-aebf-3b00374ab028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.1 - Post-shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1f168e4d-6156-4c06-a2e9-e26e46e393e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reshape_forecast_output(\n",
    "    df: pd.DataFrame,\n",
    "    response_prefixes: dict = {\n",
    "        'ultimate_claim_count': {\n",
    "            'response_type': 'count',\n",
    "            'actual_col': 'latest_view_claim_count',\n",
    "            'reported_col': 'reported_to_date_claim_count'\n",
    "        },\n",
    "        'ultimate_net_incurred': {\n",
    "            'response_type': 'net_incurred',\n",
    "            'actual_col': 'latest_view_net_claim_incurred',\n",
    "            'reported_col': 'reported_to_date_net_claim_incurred'\n",
    "        }\n",
    "    },\n",
    "    id_columns: list = ['acc_month', 'valuation_date', 'channel', 'claim_type']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reshapes ultimate model output DataFrame into long format with actual and predicted values.\n",
    "    Handles both claim count and net incurred amounts.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame.\n",
    "    - response_prefixes: Dictionary mapping response prefixes to their metadata including:\n",
    "        - response_type: Label for type of response ('count', 'net_incurred', etc.)\n",
    "        - actual_col: Column name representing actual value\n",
    "        - reported_col: Column name representing reported-to-date value\n",
    "    - id_columns: List of identifying columns to retain (e.g. ['acc_month', 'valuation_date', ...]).\n",
    "\n",
    "    Returns:\n",
    "    - A tidy DataFrame with columns: id_columns + ['model', 'actual', 'predicted', 'latest_view_*', 'reported_to_date_*', 'response']\n",
    "    \"\"\"\n",
    "    result_dfs = []\n",
    "\n",
    "    # Process each response prefix type\n",
    "    for response_prefix, config in response_prefixes.items():\n",
    "        response_type = config['response_type']\n",
    "        actual_col = config['actual_col']\n",
    "        reported_col = config['reported_col']\n",
    "        \n",
    "        # Skip if required columns are not in the DataFrame\n",
    "        if actual_col not in df.columns or reported_col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Identify ultimate columns for the given response prefix\n",
    "        ultimate_cols = [col for col in df.columns if col.startswith(response_prefix)]\n",
    "        \n",
    "        if not ultimate_cols:\n",
    "            continue\n",
    "            \n",
    "        models = [col.split('_')[-1] for col in ultimate_cols]\n",
    "\n",
    "        for model, col_name in zip(models, ultimate_cols):\n",
    "            # Make sure all required columns exist before proceeding\n",
    "            required_cols = id_columns + [col_name, actual_col, reported_col]\n",
    "            if not all(col in df.columns for col in required_cols):\n",
    "                continue\n",
    "                \n",
    "            model_df = df[required_cols].copy()\n",
    "\n",
    "            model_df['model'] = model\n",
    "            model_df['actual'] = model_df[actual_col] - model_df[reported_col]\n",
    "            model_df['predicted'] = model_df[col_name] - model_df[reported_col]\n",
    "            model_df['response'] = response_type\n",
    "\n",
    "            # Rename columns to standardised names to ensure they're consistent\n",
    "            model_df = model_df.rename(columns={\n",
    "                actual_col: 'latest_view',\n",
    "                reported_col: 'reported_to_date'\n",
    "            })\n",
    "\n",
    "            # Drop the original ultimate column\n",
    "            model_df = model_df.drop(columns=[col_name])\n",
    "            result_dfs.append(model_df)\n",
    "\n",
    "    # If no results were found, return empty DataFrame with correct columns\n",
    "    if not result_dfs:\n",
    "        return pd.DataFrame(columns=id_columns + ['model', 'actual', 'predicted', 'latest_view', 'reported_to_date', 'response'])\n",
    "    \n",
    "    # Combine all results\n",
    "    result = pd.concat(result_dfs, ignore_index=True)\n",
    "    \n",
    "    # Ensure consistent column order\n",
    "    result = result[id_columns + ['model', 'actual', 'predicted', 'latest_view', 'reported_to_date', 'response']]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46a4a4d5-d3ee-4884-8a6e-40312d5cdeb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2 - Indexation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a1b6ad-199e-4b2f-abee-c06c61f4f6b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def indexation(data, cpi, indexation_date, join_column, indexation_columns, suffix='_indexed', unindex=False):\n",
    "    \"\"\"\n",
    "    Apply indexation (e.g., CPI adjustment) to specified columns in a dataset based on a reference date.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Input DataFrame containing the data to be indexed.\n",
    "    - indexation_date (str or pd.Timestamp): Date used as the reference for indexation (e.g., valuation date).\n",
    "    - join_column (str): Column in `data` representing time (e.g., transaction date) to merge with CPI data.\n",
    "    - indexation_columns (list): List of column names in `data` to apply indexation to.\n",
    "    - suffix (str, optional): Suffix to append to indexed column names. Defaults to '_indexed'.\n",
    "    - unindex (bool, optional): If True, reverses indexation (divides by factor); if False, applies it (multiplies).\n",
    "                                Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with new indexed columns and temporary columns removed.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If columns with the `suffix` already exist in `data`.\n",
    "    - Warning: If `unindex=True` and `suffix='_indexed'` (to avoid naming confusion).\n",
    "    \"\"\"\n",
    "\n",
    "    # Create new column names by appending the suffix to the original column names\n",
    "    indexation_columns1 = [col + suffix for col in indexation_columns]\n",
    "    \n",
    "    # Check if any of the new column names already exist in the DataFrame\n",
    "    for col in indexation_columns1:\n",
    "        if col in data.columns:\n",
    "            raise ValueError(f\"Column {col} already exists in data.\")\n",
    "\n",
    "    # Create a copy of the input DataFrame to avoid modifying the original\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Add a column for the indexation reference date (converted to datetime)\n",
    "    data['cpi_valuation_quarter'] = pd.to_datetime(indexation_date)\n",
    "\n",
    "    # Merge the data with CPI data based on the join_column (e.g., transaction date)\n",
    "    # Uses merge_asof to match to the nearest CPI quarter\n",
    "    data = pd.merge_asof(\n",
    "        data.sort_values(join_column),              # Sort data by join_column (e.g., transaction date)\n",
    "        cpi.sort_values('quarter'),                 # Sort CPI data by quarter (assumes 'cpi' is a global DataFrame)\n",
    "        left_on=join_column,                        # Column in `data` to match\n",
    "        right_on='quarter',                         # Column in `cpi` to match\n",
    "        direction='nearest'                         # Match to the nearest quarter\n",
    "    ).drop(columns=['quarter']).rename(columns={'cpi': 'cpi_txn'})            # Remove the redundant 'quarter' column from CPI, # Rename CPI column to indicate transaction CPI\n",
    "\n",
    "    # Merge again to get CPI for the indexation_date (valuation date)\n",
    "    data = pd.merge_asof(\n",
    "        data.sort_values('cpi_valuation_quarter'),  # Sort by the valuation date column\n",
    "        cpi.sort_values('quarter'),                 # Sort CPI data by quarter\n",
    "        left_on='cpi_valuation_quarter',            # Match on valuation date\n",
    "        right_on='quarter',                         # Match on CPI quarter\n",
    "        direction='nearest'                         # Match to the nearest quarter\n",
    "    ).drop(columns=['quarter']).rename(columns={'cpi': 'cpi_valuation'})  # Remove the redundant 'quarter' column  # Rename CPI column to indicate valuation CPI\n",
    "\n",
    "    # Calculate the indexation factor based on whether we're indexing or unindexing\n",
    "    if unindex:\n",
    "        # If unindexing, divide transaction CPI by valuation CPI (reverse adjustment)\n",
    "        data['indexation_factor'] = data['cpi_txn'] / data['cpi_valuation']\n",
    "        # Warn if the default suffix '_indexed' is used with unindexing\n",
    "        if suffix == '_indexed':\n",
    "            raise Warning(\"Unindexing is enabled. Please change the suffix from '_indexed' to avoid confusion\")\n",
    "    else:\n",
    "        # If indexing, divide valuation CPI by transaction CPI (standard adjustment)\n",
    "        data['indexation_factor'] = data['cpi_valuation'] / data['cpi_txn']\n",
    "\n",
    "    # Drop temporary columns used for calculation\n",
    "    data.drop(columns=['cpi_txn', 'cpi_valuation', 'cpi_valuation_quarter'], inplace=True)\n",
    "\n",
    "    # Apply the indexation factor to the specified columns and create new columns with the suffix\n",
    "    data[indexation_columns1] = data[indexation_columns].multiply(data['indexation_factor'], axis=0)\n",
    "    \n",
    "    # Remove the indexation_factor column as it's no longer needed\n",
    "    data.drop(columns=['indexation_factor'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1597153-2ee1-45a1-bfe6-dad2cc628ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Diagnostic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c97baf7-619e-4ff1-af40-d1c92e4ce5fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.1 - Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6bb766fe-714a-4b21-b97a-cc5ef7630998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "def plot_claims_AVE(output, mode='net_incurred', risk_class=None, channel=None, prem_class=None, claim_type=None, models=None):\n",
    "    \"\"\"\n",
    "    Plot claims Average vs Expected with updated filtering options and MAE table.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    output : pandas DataFrame\n",
    "        The dataset containing claims data\n",
    "    mode : str, optional\n",
    "        'count' or 'net_incurred', defaults to 'net_incurred'\n",
    "    risk_class : str, optional\n",
    "        Filter by specific risk class\n",
    "    prem_class : str, optional\n",
    "        Filter by specific premium class\n",
    "    claim_type : str, optional\n",
    "        Filter by specific claim type\n",
    "    models : list, optional\n",
    "        List of models to include\n",
    "    \"\"\"\n",
    "    if mode not in ['count', 'net_incurred']:\n",
    "        raise ValueError(\"Invalid mode. Available modes are: 'count', 'net_incurred'.\")\n",
    "    \n",
    "    # Suppress warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Filter data\n",
    "    filtered_data = output[(output['response'] == mode)].copy()\n",
    "\n",
    "    # Apply new filtering options\n",
    "    if risk_class:\n",
    "        filtered_data = filtered_data[filtered_data['risk_class'] == risk_class]\n",
    "    if channel:\n",
    "        filtered_data = filtered_data[filtered_data['channel'] == channel]\n",
    "    if prem_class:\n",
    "        filtered_data = filtered_data[filtered_data['prem_class'] == prem_class]\n",
    "    if claim_type:\n",
    "        filtered_data = filtered_data[filtered_data['claim_type'] == claim_type]\n",
    "    if models:\n",
    "        filtered_data = filtered_data[filtered_data['model'].isin(models)]\n",
    "    \n",
    "    # Convert balance month to datetime\n",
    "    filtered_data['valuation_date'] = pd.to_datetime(filtered_data['valuation_date'])\n",
    "    \n",
    "    # Group by balance month and model, calculate sum of actual and predicted\n",
    "    grouped_data = filtered_data.groupby(['valuation_date', 'model']).agg({\n",
    "        'actual': 'sum',\n",
    "        'predicted': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Extract actual values (taking first model's values for reference)\n",
    "    if not grouped_data.empty and len(grouped_data['model'].unique()) > 0:\n",
    "        first_model = grouped_data['model'].unique()[0]\n",
    "        actual_values = grouped_data[grouped_data['model'] == first_model][['valuation_date', 'actual']]\n",
    "    else:\n",
    "        print(\"No data available for the selected filters.\")\n",
    "        return\n",
    "    \n",
    "    # Define cutoff date for MAE calculation (6 months from the latest date)\n",
    "    latest_date = grouped_data['valuation_date'].max()\n",
    "    cutoff_date = latest_date - pd.DateOffset(months=6)\n",
    "    \n",
    "    # Calculate MAE excluding last 6 months\n",
    "    mae_results = {}\n",
    "    for model in grouped_data['model'].unique():\n",
    "        model_data = grouped_data[grouped_data['model'] == model]\n",
    "        mae_data = model_data[model_data['valuation_date'] <= cutoff_date]\n",
    "        if not mae_data.empty:\n",
    "            mae = abs(mae_data['actual'] - mae_data['predicted']).mean()\n",
    "            mae_results[model] = mae\n",
    "    \n",
    "    # Create a subplot with 2 rows - one for the chart and one for the table\n",
    "    # Increase vertical spacing for more padding between chart and table\n",
    "    fig = make_subplots(\n",
    "        rows=2, \n",
    "        cols=1,\n",
    "        row_heights=[0.75, 0.25],  # Adjusted to give more space to the table\n",
    "        vertical_spacing=0.15,      # Increased for more padding\n",
    "        specs=[[{\"type\": \"scatter\"}], [{\"type\": \"table\"}]]\n",
    "    )\n",
    "    \n",
    "    # Add actual values trace to the first subplot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=actual_values['valuation_date'], \n",
    "            y=actual_values['actual'], \n",
    "            mode='lines+markers', \n",
    "            name='Actual',\n",
    "            line=dict(color='black', width=2),\n",
    "            hovertemplate='%{x}<br>Actual: %{y:.2f}'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Define color mapping for consistent colors across models\n",
    "    model_colors = {\n",
    "        'chainladder': 'blue',\n",
    "        'bornhuetter-ferguson': 'red',\n",
    "        'cape-cod': 'green',\n",
    "        'munich': 'purple'\n",
    "    }\n",
    "    \n",
    "    # Add predicted values traces for each model to the first subplot\n",
    "    for model in grouped_data['model'].unique():\n",
    "        model_data = grouped_data[grouped_data['model'] == model]\n",
    "        color = model_colors.get(model, None)  # Get color from mapping if available\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_data['valuation_date'], \n",
    "                y=model_data['predicted'], \n",
    "                mode='lines+markers', \n",
    "                name=f'ibnr - {model}',\n",
    "                line=dict(color=color) if color else {},  # Apply color if defined\n",
    "                hovertemplate='%{x}<br>Predicted: %{y:.2f}'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Build title with filtering information\n",
    "    title_parts = [f'Claim {mode.capitalize()}: Actual vs Predicted']\n",
    "    if risk_class:\n",
    "        title_parts.append(f'Risk Class: {risk_class}')\n",
    "    if channel:\n",
    "        title_parts.append(f'Channel: {channel}')\n",
    "    if prem_class:\n",
    "        title_parts.append(f'Premium Class: {prem_class}')\n",
    "    if claim_type:\n",
    "        title_parts.append(f'Claim Type: {claim_type}')\n",
    "    title = ' | '.join(title_parts)\n",
    "\n",
    "    # Create a table for MAE results, rankings and filters\n",
    "    # Sort models by MAE to determine ranking\n",
    "    ranked_models = sorted([(model, mae) for model, mae in mae_results.items()], key=lambda x: x[1])\n",
    "    \n",
    "    # Create ordered lists for the table\n",
    "    model_names = [model for model, _ in ranked_models]\n",
    "    mae_values = [f\"{mae:.2f}\" for _, mae in ranked_models]\n",
    "    # Create rankings (1 to n)\n",
    "    rankings = [f\"#{i+1}\" for i in range(len(ranked_models))]\n",
    "    \n",
    "    # Prepare filter information for the table\n",
    "    filter_names = []\n",
    "    filter_values = []\n",
    "    \n",
    "    # Add mode to filter info\n",
    "    filter_names.append(\"Mode\")\n",
    "    filter_values.append(mode)\n",
    "    \n",
    "    # Add other filters if they're set\n",
    "    if risk_class:\n",
    "        filter_names.append(\"Risk Class\")\n",
    "        filter_values.append(risk_class)\n",
    "    if channel:\n",
    "        filter_names.append(\"Channel\")\n",
    "        filter_values.append(channel)\n",
    "    if prem_class:\n",
    "        filter_names.append(\"Premium Class\")\n",
    "        filter_values.append(prem_class)\n",
    "    if claim_type:\n",
    "        filter_names.append(\"Claim Type\")\n",
    "        filter_values.append(claim_type)\n",
    "    if models:\n",
    "        filter_names.append(\"Models\")\n",
    "        filter_values.append(\", \".join(models) if isinstance(models, list) else models)\n",
    "    \n",
    "    # Add cutoff date information\n",
    "    filter_names.append(\"MAE Cutoff Date\")\n",
    "    filter_values.append(cutoff_date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    # Add MAE table to the second subplot - now including ranking column\n",
    "    fig.add_trace(\n",
    "        go.Table(\n",
    "            header=dict(\n",
    "                values=['Ranking', 'Model', 'MAE (excluding last 6 months)', 'Filter', 'Value'],\n",
    "                fill_color='paleturquoise',\n",
    "                align='left',\n",
    "                font=dict(size=12)\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[\n",
    "                    rankings + [\"\"] * (len(filter_names) - len(rankings)) if len(filter_names) > len(rankings) else rankings,\n",
    "                    model_names + [\"\"] * (len(filter_names) - len(model_names)) if len(filter_names) > len(model_names) else model_names,\n",
    "                    mae_values + [\"\"] * (len(filter_names) - len(mae_values)) if len(filter_names) > len(mae_values) else mae_values,\n",
    "                    filter_names + [\"\"] * (len(model_names) - len(filter_names)) if len(model_names) > len(filter_names) else filter_names,\n",
    "                    filter_values + [\"\"] * (len(model_names) - len(filter_values)) if len(model_names) > len(filter_values) else filter_values\n",
    "                ],\n",
    "                fill_color=[\n",
    "                    ['lavender'] * len(rankings) + ['white'] * (len(filter_names) - len(rankings)) if len(filter_names) > len(rankings) else ['lavender'] * len(rankings),\n",
    "                    ['lavender'] * len(model_names) + ['white'] * (len(filter_names) - len(model_names)) if len(filter_names) > len(model_names) else ['lavender'] * len(model_names),\n",
    "                    ['lavender'] * len(mae_values) + ['white'] * (len(filter_names) - len(mae_values)) if len(filter_names) > len(mae_values) else ['lavender'] * len(mae_values),\n",
    "                    ['ghostwhite'] * len(filter_names) + ['white'] * (len(model_names) - len(filter_names)) if len(model_names) > len(filter_names) else ['ghostwhite'] * len(filter_names),\n",
    "                    ['ghostwhite'] * len(filter_values) + ['white'] * (len(model_names) - len(filter_values)) if len(model_names) > len(filter_values) else ['ghostwhite'] * len(filter_values)\n",
    "                ],\n",
    "                align='left',\n",
    "                font=dict(size=11)\n",
    "            )\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Add zero line to the first subplot\n",
    "    min_date = actual_values['valuation_date'].min()\n",
    "    max_date = actual_values['valuation_date'].max()\n",
    "    \n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=min_date,\n",
    "        y0=0,\n",
    "        x1=max_date,\n",
    "        y1=0,\n",
    "        line=dict(color=\"gray\", width=1, dash=\"dash\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout with more height and adjusted margins\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=800,  # Increased height to better accommodate the expanded table\n",
    "        margin=dict(t=100, b=50, l=50, r=50),\n",
    "        template='plotly_white',\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update xaxis and yaxis for the first subplot\n",
    "    fig.update_xaxes(\n",
    "        title_text='Valuation Date',\n",
    "        tickangle=-45,\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(\n",
    "        title_text=f'Claim {mode.capitalize()}',\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add annotation for best model at the bottom left of the chart\n",
    "    if len(mae_results) > 1:\n",
    "        best_model = ranked_models[0][0]\n",
    "        best_mae = ranked_models[0][1]\n",
    "        fig.add_annotation(\n",
    "            x=0.07,  # Position at the bottom left (%)\n",
    "            y=0.45,  # Position at the bottom left (%)\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            text=f\"Best model: {best_model} (MAE: {best_mae:.2f})\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=11, color=\"green\"),\n",
    "            align=\"left\",\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.8)\",  # Semi-transparent white background\n",
    "            bordercolor=\"green\",\n",
    "            borderwidth=1,\n",
    "            borderpad=4,\n",
    "            xanchor=\"left\",\n",
    "            yanchor=\"bottom\"\n",
    "        )\n",
    "    \n",
    "    # fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7f00d1-226a-4950-b262-b0b9fefe3535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.2 - Method Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3754f81-882f-48c7-88f0-63a1ec652ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mae_excl_last_6_months(data, actual_col, pred_cols, groupby_cols, date_col='valuation_date'):\n",
    "    results = []\n",
    "\n",
    "    for keys, group in data.groupby(groupby_cols):\n",
    "        # Ensure date is datetime\n",
    "        group[date_col] = pd.to_datetime(group[date_col])\n",
    "\n",
    "        # Aggregate actual and predicted per valuation_date\n",
    "        agg_list = {'valuation_date': group[date_col].unique()}\n",
    "        agg_df = pd.DataFrame({'valuation_date': group[date_col]})\n",
    "        agg_df[actual_col] = group[actual_col]\n",
    "        for col in pred_cols:\n",
    "            agg_df[col] = group[col]\n",
    "\n",
    "        agg_df = agg_df.groupby('valuation_date').sum().reset_index()\n",
    "\n",
    "        # Define cutoff date\n",
    "        cutoff_date = agg_df['valuation_date'].max() - pd.DateOffset(months=6)\n",
    "        filtered_df = agg_df[agg_df['valuation_date'] <= cutoff_date]\n",
    "\n",
    "        # Compute MAE per model\n",
    "        mae_dict = {}\n",
    "        for col in pred_cols:\n",
    "            mae = np.abs(filtered_df[actual_col] - filtered_df[col]).mean()\n",
    "            mae_dict[col] = mae\n",
    "\n",
    "        if not mae_dict:\n",
    "            continue  # Skip if no models were evaluated\n",
    "\n",
    "        # Find best model\n",
    "        best_model = min(mae_dict, key=mae_dict.get)\n",
    "\n",
    "        # Save results\n",
    "        results.append({\n",
    "            **dict(zip(groupby_cols, keys if isinstance(keys, tuple) else [keys])),\n",
    "            **{f'mae_{col}': val for col, val in mae_dict.items()},\n",
    "            'best_model': best_model\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5a1624-6215-443f-acfb-77ac6ff0addc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_best_method(input_df, metric, groupby_cols):\n",
    "\n",
    "    if metric == 'claim_count':\n",
    "        actual_col = 'latest_view_claim_count'\n",
    "    else:\n",
    "        actual_col = 'latest_view_net_claim_incurred'\n",
    "\n",
    "    best_method = calculate_mae_excl_last_6_months(\n",
    "        input_df,\n",
    "        actual_col=actual_col,\n",
    "        pred_cols=[\n",
    "            'ultimate_' + metric + '_chainladder',\n",
    "            'ultimate_' + metric + '_bf',\n",
    "            'ultimate_' + metric + '_cc'\n",
    "        ],\n",
    "        groupby_cols=groupby_cols,\n",
    "        date_col='valuation_date'\n",
    "    )\n",
    "\n",
    "    if metric == 'claim_count':\n",
    "        best_method['response'] = 'count'\n",
    "    else:\n",
    "        best_method['response'] = 'net_incurred'\n",
    "\n",
    "    best_method['model'] = best_method['best_model'].str.split(\"_\").str[-1]\n",
    "\n",
    "    return best_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b41470c-b55b-4995-88cb-3b0ee7f84243",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Model Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a36e22ec-717a-431f-91a1-dcc80fe9c7fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.1 - Parameter Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3bf34b4-29c0-426e-bf3a-0dc31af2f7d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.2 - Compare Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82b7658f-9028-46de-8a71-e8e47219e7ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.3 - Loop through products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb834d85-c0cf-43c6-b3cc-cb42e98cae6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i, product in enumerate(product_configs, start=1):\n",
    "    print(f\"\\n=== [{i}/{len(product_configs)}] Processing Product: {product} ===\")\n",
    "\n",
    "    # 1. Setup\n",
    "    # =================================\n",
    "    print(\"     [1/8] Retrieving Configurations\")\n",
    "    curr_product_short = product_configs[product][\"product_short\"]\n",
    "    curr_claim_data = product_configs[product][\"claim_data\"]\n",
    "    curr_expo_data = product_configs[product][\"expo_data\"]\n",
    "    curr_model_data = product_configs[product][\"model_data\"]\n",
    "    curr_aggLevels = [product_configs[product][\"main_level\"]] + product_configs[product][\"sub_levels\"]\n",
    "\n",
    "    # 2. Indexation\n",
    "    # =================================\n",
    "    print(\"     [2/8] Column Indexation\")\n",
    "    cols_to_index = ['earnprem', 'net_claim_incurred', 'gross_claim_incurred', 'recoveries']\n",
    "    temp_df1 = curr_model_data.copy()\n",
    "    temp_df1[cols_to_index] = temp_df1[cols_to_index].astype(float)\n",
    "    \n",
    "    # Read cpi file and format dataframe\n",
    "    cpi_df = pd.read_csv(cpi_file_path)\n",
    "    cpi_df['quarter'] = pd.to_datetime(cpi_df['quarter'], format=\"%d/%m/%Y\")\n",
    "\n",
    "    curr_indexed_model_data = indexation(temp_df1, cpi=cpi_df, indexation_date=last_day_previous_month, join_column='obs_month', indexation_columns=cols_to_index) \n",
    "\n",
    "    # 3. Modelling\n",
    "    # =================================\n",
    "    print(\"     [3/8] Modelling\")\n",
    "    combined_df = parallel_runs(input_data=curr_indexed_model_data, triangle_groups=curr_aggLevels, valuation_dates=valuation_dates)\n",
    "\n",
    "    # 4. Reshape Output\n",
    "    # =================================\n",
    "    print(\"     [4/8] Reshaping Output\")\n",
    "    result = reshape_forecast_output(\n",
    "        df=combined_df,\n",
    "        response_prefixes={\n",
    "            'ultimate_claim_count': {\n",
    "                'response_type': 'count',\n",
    "                'actual_col': 'latest_view_claim_count',\n",
    "                'reported_col': 'reported_to_date_claim_count'\n",
    "            },\n",
    "            'ultimate_net_incurred': {\n",
    "                'response_type': 'net_incurred',\n",
    "                'actual_col': 'latest_view_net_claim_incurred',\n",
    "                'reported_col': 'reported_to_date_net_claim_incurred'\n",
    "            }\n",
    "        },\n",
    "        id_columns=['acc_month', 'valuation_date'] + curr_aggLevels\n",
    "    )\n",
    "\n",
    "    # 5. Best Method Selection\n",
    "    # =================================\n",
    "    print(\"     [5/8] Best Method Selection\")\n",
    "    claim_count_best_method = get_best_method(combined_df, 'claim_count', curr_aggLevels)\n",
    "    net_incurred_best_method = get_best_method(combined_df, 'net_incurred', curr_aggLevels)\n",
    "\n",
    "    # Union best method dataframes to be used for subsetting\n",
    "    best_method_union = pd.concat([claim_count_best_method, net_incurred_best_method]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Get best model results\n",
    "    bm_results = result.merge(\n",
    "        best_method_union[curr_aggLevels + ['model', 'response']],\n",
    "        on=curr_aggLevels + ['model', 'response'],\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # Retrieve latest valuation date for selected models\n",
    "    bm_results = bm_results[bm_results['valuation_date'] == latest_balance_date_str]\n",
    "\n",
    "    # 6. Transformations\n",
    "    # =================================\n",
    "    print(\"     [6/8] Transformations\")\n",
    "    # Consolidation\n",
    "    ultimates_pre = pd.merge(\n",
    "        curr_claim_data.groupby(['acc_month'] + curr_aggLevels)[['claim_count', 'net_claim_incurred']].sum().reset_index(),\n",
    "        bm_results,\n",
    "        on=['acc_month'] + curr_aggLevels,\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # Merge with expo_data\n",
    "    ultimates_pre = pd.merge(ultimates_pre, curr_expo_data, on=['acc_month'] + ['channel']) # + curr_aggLevels - TEMPORARY, USE CHANNEL ONLY, expo data should match granularity of aggregation\n",
    "\n",
    "    # Fill missing values and convert to int64 for relevant columns\n",
    "    columns_to_fix = ['net_claim_incurred', 'claim_count', 'predicted',  'exposure']\n",
    "    ultimates_pre[columns_to_fix] = ultimates_pre[columns_to_fix].fillna(0).astype('int64')\n",
    "    ultimates_pre['product'] = product\n",
    "\n",
    "    # Pivoting\n",
    "    pivot_indexes = ['acc_month', 'product', 'claim_count', 'net_claim_incurred', 'earnprem', 'exposure'] + curr_aggLevels\n",
    "\n",
    "    # Pivoting predicted values\n",
    "    predicted_pivot = ultimates_pre.pivot_table(index=pivot_indexes, \n",
    "                                    columns='response', \n",
    "                                    values='predicted').reset_index()\n",
    "    predicted_pivot.columns.name = None\n",
    "    predicted_pivot = predicted_pivot.rename(columns={\n",
    "        'count': 'ibnr_count',\n",
    "        'net_incurred': 'ibnr_incurred'\n",
    "    })\n",
    "\n",
    "    # Pivoting model values\n",
    "    model_pivot = ultimates_pre.pivot_table(index=pivot_indexes, \n",
    "                                columns='response', \n",
    "                                values='model', aggfunc='first').reset_index()\n",
    "    model_pivot.columns.name = None\n",
    "    model_pivot = model_pivot.rename(columns={\n",
    "        'count': 'count_model',\n",
    "        'net_incurred': 'incurred_model'\n",
    "    })\n",
    "\n",
    "    # Merge the two pivoted DataFrames\n",
    "    ultimates_df = pd.merge(predicted_pivot, model_pivot, on=pivot_indexes)\n",
    "\n",
    "    # Create ultimate count and ultimate incurred columns\n",
    "    ultimates_df['ultimate_count'] = ultimates_df['claim_count'] + ultimates_df['ibnr_count']\n",
    "    ultimates_df['ultimate_incurred'] = ultimates_df['net_claim_incurred'] + ultimates_df['ibnr_incurred']\n",
    "\n",
    "    # 7. Inflation Adjustment\n",
    "    # =================================\n",
    "    print(\"     [7/8] Inflation Adjustment\")\n",
    "    temp_df2 = ultimates_df.copy()\n",
    "\n",
    "    # Add Quarter to Ultimates Table\n",
    "    temp_df2['quarter'] = temp_df2['acc_month'].dt.to_period('Q').astype(str).str.replace('Q', 'Q', regex=False)\n",
    "\n",
    "    # CPI\n",
    "    ultimates_with_cpi = temp_df2.merge(cpi_by_quarter[['quarter', 'cpi']], on='quarter', how='left')\n",
    "\n",
    "    # Base CPI\n",
    "    base_cpi_quarter = sorted(ultimates_with_cpi['quarter'].unique())[-2] # Get second-last quarter in dataset\n",
    "    base_cpi_value = cpi_by_quarter.loc[cpi_by_quarter['quarter'] == base_cpi_quarter, 'cpi'].values[0]\n",
    "    ultimates_with_cpi['base_cpi'] = base_cpi_value\n",
    "\n",
    "    # Index Multiplier\n",
    "    ultimates_with_cpi['index_multiplier'] = ultimates_with_cpi['base_cpi'] / ultimates_with_cpi['cpi']\n",
    "\n",
    "    # Adjusted Incurreds\n",
    "    ultimates_with_cpi['adj_ultimate_incurred'] = ultimates_with_cpi['ultimate_incurred'] * ultimates_with_cpi['index_multiplier']\n",
    "    ultimates_with_cpi['adj_net_claim_incurred'] = ultimates_with_cpi['net_claim_incurred'] * ultimates_with_cpi['index_multiplier']\n",
    "\n",
    "    # 8. Output to Table\n",
    "    # =================================\n",
    "    print(\"     [8/8] Writing to databricks table\")\n",
    "    spark.createDataFrame(ultimates_with_cpi) \\\n",
    "        .write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(\"actuaries_prd.general.\" + curr_product_short.lower() + \"_ultimates_new\")\n",
    "\n",
    "    print(f\"         - Saved to: actuaries_prd.general.{curr_product_short.lower()}_ultimates_new\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IBNR Modelling Template (Standardised)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
