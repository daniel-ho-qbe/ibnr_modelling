{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6662e2af-d0f0-47e9-88a2-dcbdf85e2a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8857dfd-513a-4433-a9b3-a3fbe1e6e250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.1 - Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ccfe86d-b788-4616-8ab1-51d09db367bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.1 - Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8891e2-be97-4934-9410-ad3fc3ebf4e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Core Data Handling\n",
    "%pip install pandas --quiet\n",
    "%pip install numpy --quiet\n",
    "%pip install openpyxl --quiet\n",
    "\n",
    "# Modeling and Statistical Analysis\n",
    "%pip install statsmodels --quiet\n",
    "%pip install pygam --quiet\n",
    "\n",
    "# Actuarial Modeling\n",
    "%pip install chainladder --quiet\n",
    "%pip install sparse==0.15.5 --quiet  # Newer version conflicts with chainladder\n",
    "\n",
    "# Performance and Parallel Processing\n",
    "%pip install swifter --quiet\n",
    "%pip install joblib --quiet\n",
    "%pip install tqdm --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdda476-63bb-4a78-87bc-c6a41a14cced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.2 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "565a5873-74f1-40e8-a13a-d31fa2154148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistical Modeling\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from pygam import PoissonGAM, GAM, s, f, te\n",
    "\n",
    "# Actuarial Modeling\n",
    "import chainladder as cl\n",
    "\n",
    "# Performance and Parallel Processing\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d966c734-b116-44cb-ae99-fd58d8386298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2 - Parameterisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc6f700-cc7c-44a2-8508-f65b0b1caca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters based on assumptions \n",
    "product = \"Private Motor\"\n",
    "development_term = 24\n",
    "last_day_previous_month = datetime(2025, 5, 31)\n",
    "last_n_month_lognormal = 12\n",
    "latest_balance_date_str = last_day_previous_month.strftime('%Y-%m-%d')\n",
    "\n",
    "print(latest_balance_date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407d2ed1-dc15-4fca-8a25-3abff179c603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f464a54-8bc7-4a6c-9b76-9ae6ea45ac8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.1 - Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de8702c-ee83-41d6-82a9-e01ea3298e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Main Data\n",
    "data_claims = pd.read_feather('/Volumes/actuaries_prd/general/ibnr/data/claims_data.feather')\n",
    "data_exposure = pd.read_feather('/Volumes/actuaries_prd/general/ibnr/data/exposure_data.feather')\n",
    "data_gwp = pd.read_feather('/Volumes/actuaries_prd/general/ibnr/data/gwp_data.feather')\n",
    "data = pd.read_feather('/Volumes/actuaries_prd/general/ibnr/data/data_claims_exposure.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9874d097-d41f-402a-a063-66ec56613ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Raw data saved to table:\n",
    "\n",
    "# spark.createDataFrame(raw_data).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"actuaries_prd.general.pm_ibnr_raw_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef1aead-8e6b-4002-8eb3-eb57871a0330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e2640e47-0ade-4aa9-bde3-e52e42625bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_data = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('MM', g.loss_date) AS acc_month, \n",
    "        GREATEST(DATE_TRUNC('MM', a.observation_year_month),DATE_TRUNC('MM', g.loss_date)) AS obs_month,\n",
    "        greatest((YEAR(a.observation_year_month) - YEAR(g.loss_date)) * 12 + MONTH(a.observation_year_month) - MONTH(g.loss_date) + 1,1 ) AS dev_month,\n",
    "        CASE WHEN f.claim_code_mapped IS NULL THEN 'NAF' ELSE f.claim_code_mapped END AS claim_type,\n",
    "        g.ANZO_Super_Class AS product_group,\n",
    "        CASE WHEN coalesce(g.cell_name, 'Direct') = 'Private Motor' THEN 'Direct' else coalesce(g.cell_name, 'Direct') END AS channel,\n",
    "        SUM(a.new_claims_count) AS claim_count, \n",
    "        SUM(a.net_claims_incurred_movement_amount_gst_excl) AS net_claim_incurred,\n",
    "        SUM(a.gross_claims_incurred_movement_amount_gst_excl) as gross_claim_incurred,\n",
    "        SUM(a.claim_recoveries_movement_amount_gst_excl) as recoveries\n",
    "    FROM \n",
    "        cds_prd.cds.claim_claim_transactionmonth_financialcounts a \n",
    "    LEFT JOIN\n",
    "        cds_prd.rds.claim_claim_transactiondaily_financialcounts_detail g on a.claim_fkey = g.claim_origin_key\n",
    "    LEFT JOIN \n",
    "        ids_prd.ref.ref_cause_of_loss c ON a.cause_of_loss_fkey = c.origin_key\n",
    "    LEFT JOIN\n",
    "        actuaries_prd.general.pm_claimtype_mapping f on c.claim_code = f.claim_code\n",
    "    LEFT JOIN \n",
    "        (select distinct PolicyNumber,ReferenceProductCode_Ext FROM staging_prd.gw.pc_policyperiod) h ON g.Policy_Number = h.PolicyNumber\n",
    "    WHERE \n",
    "        YEAR(g.loss_date) >=2017\n",
    "        AND g.anzo_super_class = '{product}'\n",
    "        AND a.observation_year_month <= '{latest_balance_date_str}'\n",
    "        AND g.loss_date <= '{latest_balance_date_str}'\n",
    "        AND c.incident_description is not null\n",
    "        AND coalesce(Distribution_Area, 'NULL') <> 'Motorcycle'\n",
    "        AND (REPLACE(g.account_number, '||', '') not in ('AVMYTESLA', '56PORSCHN', '56PORSCHR','1Q0009136','1Q0009137','1Q0009138','1Q0009139','1Q0009140','1Q0009142','1Q0009162','1Q0009168','1Q0009178','40STELLA') or g.account_number is null)\n",
    "        AND coalesce(ReferenceProductCode_Ext, 'NULL') <>'CVT'\n",
    "        AND coalesce(claims_category) <>'Catastrophe'\n",
    "    GROUP BY \n",
    "        all\n",
    "    ORDER BY \n",
    "        all\n",
    "    \"\"\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3ee1a6d8-4f6a-4473-90d7-d7e10962c289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expo_data = spark.sql(f\"\"\"\n",
    "WITH policy_transformed AS (\n",
    "    SELECT DISTINCT \n",
    "        CONCAT(COMPANY_CODE, POLICY_BR, POLICY_NO, POLICY_TYP) AS Policy, \n",
    "        CONCAT(ACCOUNT_BR, ACCOUNT_NO) AS Account,\n",
    "        ROW_NUMBER() OVER (PARTITION BY COMPANY_CODE, POLICY_BR, POLICY_NO, POLICY_TYP ORDER BY DATE_EFFECT DESC) AS rn\n",
    "    FROM STAGING_PRD.EVO.tal_evo_polh_01\n",
    "    WHERE ACCOUNT_BR IS NOT NULL AND ACCOUNT_NO IS NOT NULL\n",
    "),\n",
    "expo_data_raw AS (\n",
    "    SELECT  \n",
    "        DATE_TRUNC('MM', exp_start) AS acc_month,\n",
    "        CASE \n",
    "            WHEN channel = 'ANZ' THEN 'ANZ'\n",
    "            WHEN channel = 'BD' THEN 'Broker Distribution'\n",
    "            WHEN channel = 'ELDERS' THEN 'Elders'\n",
    "            WHEN channel = 'FIOTHER' THEN 'FI Other'\n",
    "            WHEN channel = 'M TRADE' THEN 'Motor Trades'\n",
    "            WHEN channel IN ('DIRECT', 'AUSPOST', 'KOGAN') THEN 'Direct'\n",
    "        END AS channel,\n",
    "        F_policyno AS policyno,\n",
    "        earnprem AS earnprem,\n",
    "        exposure AS exposure\n",
    "    FROM actuarial_onprem_sqlserver.dbo.fact_mpa_prem\n",
    "    WHERE exp_start >= '2017-01-01' AND exp_start <= '{latest_balance_date_str}'\n",
    ")\n",
    "SELECT e.acc_month, e.channel, SUM(e.earnprem) AS earnprem, SUM(e.exposure) AS exposure\n",
    "FROM expo_data_raw e\n",
    "LEFT JOIN policy_transformed p \n",
    "    ON e.policyno = p.Policy AND p.rn = 1\n",
    "WHERE p.Account IS NULL OR p.Account NOT IN ('AVMYTESLA', '56PORSCHN', '56PORSCHR','1Q0009136','1Q0009137',\n",
    "                      '1Q0009138','1Q0009139','1Q0009140','1Q0009142','1Q0009162',\n",
    "                      '1Q0009168','1Q0009178','40STELLA')\n",
    "GROUP BY e.acc_month, e.channel\n",
    "order by 1,2\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "736b15e3-78b0-40bc-82d1-07b7a6675cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2 - Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "418de623-d859-409d-a4ec-9f8d3aad4f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "data_date = '2025-05-31'\n",
    "development_period_end = 24\n",
    "n_periods = 12\n",
    "triangle_groups = ['channel', 'claim_type']\n",
    "acc_month_start = pd.to_datetime('2017-01-01')\n",
    "\n",
    "# Date Range\n",
    "valuation_dates = pd.date_range(start=\"2019-01-31\", end=data_date, freq='ME').strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "# Pre-filtering\n",
    "input_data = data[\n",
    "    (data['dev_month'] <= development_term) &\n",
    "    (data['acc_month'] >= acc_month_start)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6169317-c05b-406c-b979-2f199d8e0d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1083bf6a-d13c-4600-b9f7-1eaf9fb6a549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(type(data['dev_month'].iloc[0]))\n",
    "print(sorted(data['dev_month'].unique()))\n",
    "print(sorted(input_data['dev_month'].unique()))\n",
    "\n",
    "\n",
    "sample_data = input_data[\n",
    "    (input_data['channel'] == 'Broker Distribution') &\n",
    "    ((input_data['claim_type'] == 'AFM') + (input_data['claim_type'] == 'WND'))\n",
    "]\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e117841-06f2-44de-a88c-89d6bdff2362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76610c49-3bf6-4418-8a08-84ef0695c2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.1 - Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d03eef5-00a3-4126-976a-6f94cc02918f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_models(input_data, valuation_date, development_period_end = 24, triangle_groups = ['channel', 'claim_type'], n_periods = 12):\n",
    "\n",
    "    # 1. Data Processing\n",
    "    # =================================\n",
    "\n",
    "    data_hidden = input_data[\n",
    "        (input_data['obs_month'] <= valuation_date) &\n",
    "        (input_data['acc_month'] <= valuation_date)\n",
    "    ] \n",
    "\n",
    "    data_full = input_data.copy()\n",
    "\n",
    "    # Create triangle on partial data\n",
    "    triangle_combined = cl.Triangle(\n",
    "        data_hidden,\n",
    "        origin=\"acc_month\",\n",
    "        development=\"obs_month\",\n",
    "        columns=[\n",
    "            'claim_count',\n",
    "            'net_claim_incurred',\n",
    "            'gross_claim_incurred',\n",
    "            'net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed',\n",
    "            'earnprem_indexed',\n",
    "            'exposure',\n",
    "            'recoveries',\n",
    "            'earnprem'\n",
    "        ],\n",
    "        index=triangle_groups,\n",
    "        cumulative=False\n",
    "    ).incr_to_cum()\n",
    "\n",
    "    # Transformations\n",
    "    triangle_combined['frequency'] = triangle_combined['claim_count'] / triangle_combined['exposure']\n",
    "    triangle_combined['gross_cost_per_policy'] = triangle_combined['gross_claim_incurred'] / triangle_combined['exposure']\n",
    "    triangle_combined['gross_cost_per_policy_indexed'] = triangle_combined['gross_claim_incurred_indexed'] / triangle_combined['exposure']\n",
    "    triangle_combined['net_cost_per_policy'] = triangle_combined['net_claim_incurred'] / triangle_combined['exposure']\n",
    "    triangle_combined['net_cost_per_policy_indexed'] = triangle_combined['net_claim_incurred_indexed'] / triangle_combined['exposure']\n",
    "    triangle_combined['net_loss_ratio'] = triangle_combined['net_claim_incurred'] / triangle_combined['earnprem']\n",
    "    triangle_combined['net_loss_ratio_indexed'] = triangle_combined['net_claim_incurred_indexed'] / triangle_combined['earnprem_indexed']\n",
    "\n",
    "    # ------------------------------------\n",
    "    # FULL TRIANGLE (FOR ACTUAL RESULTS)\n",
    "    # ------------------------------------\n",
    "    triangle_combined_full = cl.Triangle(\n",
    "        data_full,\n",
    "        origin='acc_month',\n",
    "        development='obs_month',\n",
    "        columns=[\n",
    "            'claim_count',\n",
    "            'net_claim_incurred',\n",
    "            'gross_claim_incurred',\n",
    "            'net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed',\n",
    "            'earnprem_indexed',\n",
    "            'exposure',\n",
    "            'recoveries',\n",
    "            'earnprem'\n",
    "        ],\n",
    "        index=triangle_groups,\n",
    "        cumulative=False,\n",
    "    ).incr_to_cum()\n",
    "\n",
    "    output_actual_results = triangle_combined_full[\n",
    "        [\n",
    "            \"net_claim_incurred\",\n",
    "            \"gross_claim_incurred\",\n",
    "            \"claim_count\",\n",
    "            \"net_claim_incurred_indexed\",\n",
    "            \"gross_claim_incurred_indexed\",\n",
    "            \"recoveries_indexed\",\n",
    "        ]\n",
    "    ].latest_diagonal.to_frame().reset_index().drop(\n",
    "        columns=['valuation'] # Drop valuation and manually append after\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'origin' : 'acc_month',\n",
    "            'claim_count': 'latest_view_claim_count',\n",
    "            'net_claim_incurred': 'latest_view_net_claim_incurred',\n",
    "            'gross_claim_incurred': 'latest_view_gross_claim_incurred',\n",
    "            'recoveries': 'latest_view_recoveries',\n",
    "            'net_claim_incurred_indexed': 'latest_view_net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed': 'latest_view_gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed': 'latest_view_recoveries_indexed'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 2.1 Model Training - Claim Count\n",
    "    # =================================\n",
    "\n",
    "    # Build initial development triangle and replace the development factors to assume fully developed in {development_period_end} periods\n",
    "    claim_count_development_factors  = cl.Development(n_periods=n_periods).fit_transform(triangle_combined[['claim_count','frequency']])\n",
    "    for i in range(0, claim_count_development_factors.ldf_.values.shape[0]):\n",
    "        claim_count_development_factors.ldf_.values[i][:,0,development_period_end:] = 1\n",
    "        claim_count_development_factors.cdf_.values[i][:,0,development_period_end:] = 1\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 1 - Chainladder\n",
    "    # ------------------------------------\n",
    "    claim_count_chainladder = cl.Chainladder().fit(claim_count_development_factors)\n",
    "\n",
    "    weights = triangle_combined['exposure'].latest_diagonal\n",
    "    weights /= np.sum(weights, axis=2, keepdims=True)\n",
    "\n",
    "    apriori_claim_count = np.sum((claim_count_chainladder.ultimate_['frequency'] * weights).iloc[:, :, -12:, :], axis=2, keepdims=True) / np.sum(weights.iloc[:, :, -12:, :], axis=2, keepdims=True)\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 2 - Bornhuetter-Ferguson\n",
    "    # ------------------------------------\n",
    "    claim_count_bf = cl.BornhuetterFerguson(\n",
    "        apriori=  apriori_claim_count\n",
    "    ).fit(\n",
    "        triangle_combined[['claim_count','exposure']]\n",
    "        , sample_weight = triangle_combined['exposure'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 3 - Cape Cod\n",
    "    # ------------------------------------\n",
    "    claim_count_cc = cl.CapeCod().fit(\n",
    "        triangle_combined[['claim_count','exposure']],\n",
    "        sample_weight=triangle_combined['exposure'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    output_triangle = triangle_combined[\n",
    "        [\n",
    "            \"exposure\",\n",
    "            \"earnprem\",\n",
    "            \"earnprem_indexed\",\n",
    "            \"claim_count\",\n",
    "            \"net_claim_incurred\",\n",
    "            \"gross_claim_incurred\",\n",
    "            \"recoveries\",\n",
    "            \"net_claim_incurred_indexed\",\n",
    "            \"gross_claim_incurred_indexed\",\n",
    "            \"recoveries_indexed\",\n",
    "        ]\n",
    "    ].latest_diagonal.to_frame().reset_index().drop(\n",
    "        columns=['valuation']\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'origin' : 'acc_month',\n",
    "            'claim_count': 'reported_to_date_claim_count',\n",
    "            'net_claim_incurred': 'reported_to_date_net_claim_incurred',\n",
    "            'gross_claim_incurred': 'reported_to_date_gross_claim_incurred',\n",
    "            'recoveries': 'reported_to_date_recoveries',\n",
    "            'net_claim_incurred_indexed': 'reported_to_date_net_claim_incurred_indexed',\n",
    "            'gross_claim_incurred_indexed': 'reported_to_date_gross_claim_incurred_indexed',\n",
    "            'recoveries_indexed': 'reported_to_date_recoveries_indexed'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 2.2 Model Training - Net Incurred\n",
    "    # =================================\n",
    "\n",
    "    # Build initial development triangle and replace the development factors to assume fully developed in {development_period_end} periods\n",
    "    net_incurred_development_factors  = cl.Development(n_periods=n_periods).fit_transform(triangle_combined[['net_claim_incurred','net_cost_per_policy','net_loss_ratio']])\n",
    "    for i in range(0, net_incurred_development_factors.ldf_.values.shape[0]):\n",
    "        net_incurred_development_factors.ldf_.values[i][:,0,development_period_end:] = 1\n",
    "        net_incurred_development_factors.cdf_.values[i][:,0,development_period_end:] = 1\n",
    "\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 1 - Chainladder\n",
    "    # ------------------------------------\n",
    "    \n",
    "    # Fit a chainladder model using the adjusted development factors from dev object\n",
    "    net_incurred_chainladder = cl.Chainladder().fit(net_incurred_development_factors)\n",
    "\n",
    "    weights = triangle_combined['earnprem'].latest_diagonal\n",
    "    weights /= np.sum(weights, axis=2, keepdims=True)\n",
    "\n",
    "    apriori_net_incurred = np.sum((net_incurred_chainladder.ultimate_['net_loss_ratio'] * weights).iloc[:, :, -12:, :], axis=2, keepdims=True) / np.sum(weights.iloc[:, :, -12:, :], axis=2, keepdims=True)\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 2 - Bornhuetter-Ferguson\n",
    "    # ------------------------------------\n",
    "\n",
    "    net_incurred_bf = cl.BornhuetterFerguson(\n",
    "        apriori=apriori_net_incurred\n",
    "    ).fit(\n",
    "        net_incurred_development_factors, \n",
    "        sample_weight=triangle_combined['earnprem'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODEL 3 - Cape Cod\n",
    "    # ------------------------------------\n",
    "\n",
    "    net_incurred_cc = cl.CapeCod().fit(\n",
    "        net_incurred_development_factors,  # Use the same dev factors!\n",
    "        sample_weight=triangle_combined['earnprem'].latest_diagonal\n",
    "    )\n",
    "\n",
    "    # 3. Joins to Final Dataframe\n",
    "    # =================================\n",
    "\n",
    "    # ------------------------------------\n",
    "    # FULL TRIANGLE - ACTUAL RESULTS\n",
    "    # ------------------------------------\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_triangle,\n",
    "        output_actual_results,\n",
    "        left_on  = triangle_groups + ['acc_month'],\n",
    "        right_on =  triangle_groups + ['acc_month'],\n",
    "        how='left'\n",
    "    ).drop(\n",
    "        columns=['exposure']\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # COUNT - CHAIN LADDER\n",
    "    # ------------------------------------\n",
    "    output_claim_count_chainladder = claim_count_chainladder.ultimate_.to_frame().reset_index(\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'claim_count': 'ultimate_claim_count_chainladder'\n",
    "            ,'origin' : 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_claim_count_chainladder,\n",
    "        left_on  = triangle_groups + ['acc_month'],\n",
    "        right_on =  triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # NET INCURRED - CHAIN LADDER\n",
    "    # ------------------------------------\n",
    "\n",
    "    output_net_incurred_chainladder = net_incurred_chainladder.ultimate_.to_frame().reset_index().drop(\n",
    "        columns=['valuation', 'net_cost_per_policy']\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'net_claim_incurred': 'ultimate_net_incurred_chainladder',\n",
    "            'origin': 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_net_incurred_chainladder,\n",
    "        left_on = triangle_groups + ['acc_month'],\n",
    "        right_on = triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # COUNT - BORNHUETTER-FERGUSON\n",
    "    # ------------------------------------\n",
    "    output_claim_count_bf = claim_count_bf.ultimate_.to_frame().reset_index(\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'claim_count': 'ultimate_claim_count_bf'\n",
    "            ,'origin' : 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_claim_count_bf,\n",
    "        left_on  = triangle_groups + ['acc_month'],\n",
    "        right_on =  triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # NET INCURRED - BORNHUETTER-FERGUSON\n",
    "    # ------------------------------------\n",
    "\n",
    "    output_net_incurred_bf = net_incurred_bf.ultimate_.to_frame().reset_index().drop(\n",
    "        columns=['valuation', 'net_cost_per_policy', 'net_loss_ratio']\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'net_claim_incurred': 'ultimate_net_incurred_bf',\n",
    "            'origin': 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_net_incurred_bf,\n",
    "        left_on = triangle_groups + ['acc_month'],\n",
    "        right_on = triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # COUNT - CAPE COD\n",
    "    # ------------------------------------\n",
    "    output_claim_count_cc = claim_count_cc.ultimate_.to_frame().reset_index().rename(\n",
    "        columns={\n",
    "            'claim_count': 'ultimate_claim_count_cc',\n",
    "            'origin': 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_claim_count_cc,\n",
    "        left_on=triangle_groups + ['acc_month'],\n",
    "        right_on=triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # NET INCURRED - CAPE COD\n",
    "    # ------------------------------------\n",
    "\n",
    "    output_net_incurred_cc = net_incurred_cc.ultimate_.to_frame().reset_index().drop(\n",
    "        columns=['valuation', 'net_cost_per_policy', 'net_loss_ratio']\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'net_claim_incurred': 'ultimate_net_incurred_cc',\n",
    "            'origin': 'acc_month'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        output_net_incurred_cc,\n",
    "        left_on=triangle_groups + ['acc_month'],\n",
    "        right_on=triangle_groups + ['acc_month'],\n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    # Final Output\n",
    "    output_results['valuation_date'] = pd.to_datetime(valuation_date)\n",
    "\n",
    "    # Aggregate required columns from the original data\n",
    "    additional_fields = input_data.groupby(triangle_groups + ['acc_month'], as_index=False).agg({\n",
    "        'product_group': 'first'  # Assuming 'product' is constant within each group\n",
    "    })\n",
    "\n",
    "    # Merge into the final output\n",
    "    output_results = pd.merge(\n",
    "        output_results,\n",
    "        additional_fields,\n",
    "        on=triangle_groups + ['acc_month'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    return output_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba86bb0b-f0e5-4cdd-bd07-bcb78e7aacf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.2 - Parallel Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420777bd-1866-42cc-befd-949764da5999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    # Run in parallel\n",
    "    valuation_data = Parallel(n_jobs=-1)(delayed(\n",
    "        build_models\n",
    "    )(\n",
    "        sample_data,\n",
    "        date,\n",
    "        24,\n",
    "        ['channel', 'claim_type'],\n",
    "        12\n",
    "    ) for date in tqdm(valuation_dates, desc=\"Processing valuation dates\"))\n",
    "\n",
    "    # Combine results into a single DataFrame\n",
    "    combined_df = pd.concat(valuation_data, ignore_index=True)\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790c6bea-f750-4535-aebf-3b00374ab028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.3 - Post-shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1f168e4d-6156-4c06-a2e9-e26e46e393e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reshape_forecast_output(\n",
    "    df: pd.DataFrame,\n",
    "    response_prefixes: dict = {\n",
    "        'ultimate_claim_count': {\n",
    "            'response_type': 'count',\n",
    "            'actual_col': 'latest_view_claim_count',\n",
    "            'reported_col': 'reported_to_date_claim_count'\n",
    "        },\n",
    "        'ultimate_net_incurred': {\n",
    "            'response_type': 'net_incurred',\n",
    "            'actual_col': 'latest_view_net_claim_incurred',\n",
    "            'reported_col': 'reported_to_date_net_claim_incurred'\n",
    "        }\n",
    "    },\n",
    "    id_columns: list = ['acc_month', 'valuation_date', 'channel', 'claim_type']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reshapes ultimate model output DataFrame into long format with actual and predicted values.\n",
    "    Handles both claim count and net incurred amounts.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame.\n",
    "    - response_prefixes: Dictionary mapping response prefixes to their metadata including:\n",
    "        - response_type: Label for type of response ('count', 'net_incurred', etc.)\n",
    "        - actual_col: Column name representing actual value\n",
    "        - reported_col: Column name representing reported-to-date value\n",
    "    - id_columns: List of identifying columns to retain (e.g. ['acc_month', 'valuation_date', ...]).\n",
    "\n",
    "    Returns:\n",
    "    - A tidy DataFrame with columns: id_columns + ['model', 'actual', 'predicted', 'latest_view_*', 'reported_to_date_*', 'response']\n",
    "    \"\"\"\n",
    "    result_dfs = []\n",
    "\n",
    "    # Process each response prefix type\n",
    "    for response_prefix, config in response_prefixes.items():\n",
    "        response_type = config['response_type']\n",
    "        actual_col = config['actual_col']\n",
    "        reported_col = config['reported_col']\n",
    "        \n",
    "        # Skip if required columns are not in the DataFrame\n",
    "        if actual_col not in df.columns or reported_col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Identify ultimate columns for the given response prefix\n",
    "        ultimate_cols = [col for col in df.columns if col.startswith(response_prefix)]\n",
    "        \n",
    "        if not ultimate_cols:\n",
    "            continue\n",
    "            \n",
    "        models = [col.split('_')[-1] for col in ultimate_cols]\n",
    "\n",
    "        for model, col_name in zip(models, ultimate_cols):\n",
    "            # Make sure all required columns exist before proceeding\n",
    "            required_cols = id_columns + [col_name, actual_col, reported_col]\n",
    "            if not all(col in df.columns for col in required_cols):\n",
    "                continue\n",
    "                \n",
    "            model_df = df[required_cols].copy()\n",
    "\n",
    "            model_df['model'] = model\n",
    "            model_df['actual'] = model_df[actual_col] - model_df[reported_col]\n",
    "            model_df['predicted'] = model_df[col_name] - model_df[reported_col]\n",
    "            model_df['response'] = response_type\n",
    "\n",
    "            # Rename columns to standardised names to ensure they're consistent\n",
    "            model_df = model_df.rename(columns={\n",
    "                actual_col: 'latest_view',\n",
    "                reported_col: 'reported_to_date'\n",
    "            })\n",
    "\n",
    "            # Drop the original ultimate column\n",
    "            model_df = model_df.drop(columns=[col_name])\n",
    "            result_dfs.append(model_df)\n",
    "\n",
    "    # If no results were found, return empty DataFrame with correct columns\n",
    "    if not result_dfs:\n",
    "        return pd.DataFrame(columns=id_columns + ['model', 'actual', 'predicted', 'latest_view', 'reported_to_date', 'response'])\n",
    "    \n",
    "    # Combine all results\n",
    "    result = pd.concat(result_dfs, ignore_index=True)\n",
    "    \n",
    "    # Ensure consistent column order\n",
    "    result = result[id_columns + ['model', 'actual', 'predicted', 'latest_view', 'reported_to_date', 'response']]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "080176b1-3230-406b-a7f2-52a86862aa9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def reshape_forecast_output(\n",
    "#     df: pd.DataFrame,\n",
    "#     response_prefixes: dict = {\n",
    "#         'ultimate_claim_count': {\n",
    "#             'response_type': 'count',\n",
    "#             'actual_col': 'latest_view_claim_count',\n",
    "#             'reported_col': 'reported_to_date_claim_count'\n",
    "#         },\n",
    "#         'ultimate_net_incurred': {\n",
    "#             'response_type': 'net_incurred',\n",
    "#             'actual_col': 'latest_view_net_claim_incurred',\n",
    "#             'reported_col': 'reported_to_date_net_claim_incurred'\n",
    "#         }\n",
    "#     },\n",
    "#     id_columns: list = ['acc_month', 'channel', 'claim_type'],\n",
    "#     additional_columns: list = ['exposure', 'earnprem', 'product_group']\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Reshapes ultimate model output DataFrame into long format with actual and predicted values.\n",
    "#     Keeps extra business-related columns in final output.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df: Input DataFrame.\n",
    "#     - response_prefixes: Dictionary mapping response prefixes to their metadata.\n",
    "#     - id_columns: List of identifying columns to retain.\n",
    "#     - additional_columns: List of additional columns to retain in the reshaped output.\n",
    "\n",
    "#     Returns:\n",
    "#     - A tidy DataFrame with columns: id_columns + additional_columns + ['model', 'actual', 'predicted', 'latest_view', 'reported_to_date', 'response']\n",
    "#     \"\"\"\n",
    "#     result_dfs = []\n",
    "\n",
    "#     for response_prefix, config in response_prefixes.items():\n",
    "#         response_type = config['response_type']\n",
    "#         actual_col = config['actual_col']\n",
    "#         reported_col = config['reported_col']\n",
    "        \n",
    "#         if actual_col not in df.columns or reported_col not in df.columns:\n",
    "#             continue\n",
    "        \n",
    "#         ultimate_cols = [col for col in df.columns if col.startswith(response_prefix)]\n",
    "#         if not ultimate_cols:\n",
    "#             continue\n",
    "            \n",
    "#         models = [col.split('_')[-1] for col in ultimate_cols]\n",
    "\n",
    "#         for model, col_name in zip(models, ultimate_cols):\n",
    "#             required_cols = id_columns + [col_name, actual_col, reported_col]\n",
    "#             optional_additional = [col for col in additional_columns if col in df.columns]\n",
    "#             full_columns = required_cols + optional_additional\n",
    "\n",
    "#             if not all(col in df.columns for col in required_cols):\n",
    "#                 continue\n",
    "\n",
    "#             model_df = df[full_columns].copy()\n",
    "#             model_df['model'] = model\n",
    "#             model_df['actual'] = model_df[actual_col] - model_df[reported_col]\n",
    "#             model_df['predicted'] = model_df[col_name] - model_df[reported_col]\n",
    "#             model_df['response'] = response_type\n",
    "\n",
    "#             model_df = model_df.rename(columns={\n",
    "#                 actual_col: 'latest_view',\n",
    "#                 reported_col: 'reported_to_date'\n",
    "#             })\n",
    "\n",
    "#             model_df = model_df.drop(columns=[col_name])\n",
    "#             result_dfs.append(model_df)\n",
    "\n",
    "#     if not result_dfs:\n",
    "#         return pd.DataFrame(columns=id_columns + ['model', 'actual', 'predicted', 'latest_view', 'reported_to_date', 'response'] + additional_columns)\n",
    "\n",
    "#     result = pd.concat(result_dfs, ignore_index=True)\n",
    "\n",
    "#     # Ensure consistent column order\n",
    "#     optional_additional = [col for col in additional_columns if col in result.columns]\n",
    "#     result = result[id_columns + optional_additional + ['model', 'actual', 'predicted', 'latest_view', 'reported_to_date', 'response']]\n",
    "\n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b898c5bc-ed5c-4def-aba2-6b6171542402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = reshape_forecast_output(\n",
    "    df=combined_df,\n",
    "    response_prefixes={\n",
    "        'ultimate_claim_count': {\n",
    "            'response_type': 'count',\n",
    "            'actual_col': 'latest_view_claim_count',\n",
    "            'reported_col': 'reported_to_date_claim_count'\n",
    "        },\n",
    "        'ultimate_net_incurred': {\n",
    "            'response_type': 'net_incurred',\n",
    "            'actual_col': 'latest_view_net_claim_incurred',\n",
    "            'reported_col': 'reported_to_date_net_claim_incurred'\n",
    "        }\n",
    "    },\n",
    "    id_columns=['acc_month', 'valuation_date', 'channel', 'claim_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "161775f4-e6ed-47c2-aaa2-9d1d75e180e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7f00d1-226a-4950-b262-b0b9fefe3535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.4 - Method Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c3754f81-882f-48c7-88f0-63a1ec652ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mae_excl_last_6_months(data, actual_col, pred_cols, groupby_cols, date_col='valuation_date'):\n",
    "    results = []\n",
    "\n",
    "    for keys, group in data.groupby(groupby_cols):\n",
    "        # Ensure date is datetime\n",
    "        group[date_col] = pd.to_datetime(group[date_col])\n",
    "\n",
    "        # Aggregate actual and predicted per valuation_date\n",
    "        agg_list = {'valuation_date': group[date_col].unique()}\n",
    "        agg_df = pd.DataFrame({'valuation_date': group[date_col]})\n",
    "        agg_df[actual_col] = group[actual_col]\n",
    "        for col in pred_cols:\n",
    "            agg_df[col] = group[col]\n",
    "\n",
    "        agg_df = agg_df.groupby('valuation_date').sum().reset_index()\n",
    "\n",
    "        # Define cutoff date\n",
    "        cutoff_date = agg_df['valuation_date'].max() - pd.DateOffset(months=6)\n",
    "        filtered_df = agg_df[agg_df['valuation_date'] <= cutoff_date]\n",
    "\n",
    "        # Compute MAE per model\n",
    "        mae_dict = {}\n",
    "        for col in pred_cols:\n",
    "            mae = np.abs(filtered_df[actual_col] - filtered_df[col]).mean()\n",
    "            mae_dict[col] = mae\n",
    "\n",
    "        if not mae_dict:\n",
    "            continue  # Skip if no models were evaluated\n",
    "\n",
    "        # Find best model\n",
    "        best_model = min(mae_dict, key=mae_dict.get)\n",
    "\n",
    "        # Save results\n",
    "        results.append({\n",
    "            **dict(zip(groupby_cols, keys if isinstance(keys, tuple) else [keys])),\n",
    "            **{f'mae_{col}': val for col, val in mae_dict.items()},\n",
    "            'best_model': best_model\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072588a5-86d7-4197-aab1-97e48807d739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.4.1 - Claim Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "398dee48-0af5-4e6d-8504-84013d9628b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Claim Count Method Selection\n",
    "claim_count_best_method = calculate_mae_excl_last_6_months(\n",
    "    combined_df,\n",
    "    actual_col='latest_view_claim_count',\n",
    "    pred_cols=[\n",
    "        'ultimate_claim_count_chainladder',\n",
    "        'ultimate_claim_count_bf',\n",
    "        'ultimate_claim_count_cc'\n",
    "    ],\n",
    "    # groupby_cols=['channel', 'claim_type']\n",
    "    groupby_cols=['channel'],\n",
    "    date_col='valuation_date'\n",
    ")\n",
    "\n",
    "# Break down best_model into model and response\n",
    "claim_count_best_method['response'] = 'count'\n",
    "claim_count_best_method['model'] = claim_count_best_method['best_model'].str.split(\"_\").str[-1]\n",
    "\n",
    "claim_count_best_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2692b840-2b9e-4d0a-9ea3-e23c076ea55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.4.2 - Net Incurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dbca5e48-0032-4c55-89a2-917bd182cf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Net Incurred Method Selection\n",
    "net_incurred_best_method = calculate_mae_excl_last_6_months(\n",
    "    combined_df,\n",
    "    actual_col='latest_view_net_claim_incurred',\n",
    "    pred_cols=[\n",
    "        'ultimate_net_incurred_chainladder',\n",
    "        'ultimate_net_incurred_bf',\n",
    "        'ultimate_net_incurred_cc'\n",
    "    ],\n",
    "    # groupby_cols=['channel', 'claim_type']\n",
    "    groupby_cols=['channel'],\n",
    "    date_col='valuation_date'\n",
    ")\n",
    "\n",
    "# Break down best_model into model and response\n",
    "net_incurred_best_method['response'] = 'net_incurred'\n",
    "net_incurred_best_method['model'] = net_incurred_best_method['best_model'].str.split(\"_\").str[-1]\n",
    "\n",
    "net_incurred_best_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1597153-2ee1-45a1-bfe6-dad2cc628ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c97baf7-619e-4ff1-af40-d1c92e4ce5fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.1 - Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb766fe-714a-4b21-b97a-cc5ef7630998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "def plot_claims_AVE(output, mode='net_incurred', risk_class=None, channel=None, prem_class=None, claim_type=None, models=None):\n",
    "    \"\"\"\n",
    "    Plot claims Average vs Expected with updated filtering options and MAE table.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    output : pandas DataFrame\n",
    "        The dataset containing claims data\n",
    "    mode : str, optional\n",
    "        'count' or 'net_incurred', defaults to 'net_incurred'\n",
    "    risk_class : str, optional\n",
    "        Filter by specific risk class\n",
    "    prem_class : str, optional\n",
    "        Filter by specific premium class\n",
    "    claim_type : str, optional\n",
    "        Filter by specific claim type\n",
    "    models : list, optional\n",
    "        List of models to include\n",
    "    \"\"\"\n",
    "    if mode not in ['count', 'net_incurred']:\n",
    "        raise ValueError(\"Invalid mode. Available modes are: 'count', 'net_incurred'.\")\n",
    "    \n",
    "    # Suppress warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Filter data\n",
    "    filtered_data = output[(output['response'] == mode)].copy()\n",
    "\n",
    "    # Apply new filtering options\n",
    "    if risk_class:\n",
    "        filtered_data = filtered_data[filtered_data['risk_class'] == risk_class]\n",
    "    if channel:\n",
    "        filtered_data = filtered_data[filtered_data['channel'] == channel]\n",
    "    if prem_class:\n",
    "        filtered_data = filtered_data[filtered_data['prem_class'] == prem_class]\n",
    "    if claim_type:\n",
    "        filtered_data = filtered_data[filtered_data['claim_type'] == claim_type]\n",
    "    if models:\n",
    "        filtered_data = filtered_data[filtered_data['model'].isin(models)]\n",
    "    \n",
    "    # Convert balance month to datetime\n",
    "    filtered_data['valuation_date'] = pd.to_datetime(filtered_data['valuation_date'])\n",
    "    \n",
    "    # Group by balance month and model, calculate sum of actual and predicted\n",
    "    grouped_data = filtered_data.groupby(['valuation_date', 'model']).agg({\n",
    "        'actual': 'sum',\n",
    "        'predicted': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Extract actual values (taking first model's values for reference)\n",
    "    if not grouped_data.empty and len(grouped_data['model'].unique()) > 0:\n",
    "        first_model = grouped_data['model'].unique()[0]\n",
    "        actual_values = grouped_data[grouped_data['model'] == first_model][['valuation_date', 'actual']]\n",
    "    else:\n",
    "        print(\"No data available for the selected filters.\")\n",
    "        return\n",
    "    \n",
    "    # Define cutoff date for MAE calculation (6 months from the latest date)\n",
    "    latest_date = grouped_data['valuation_date'].max()\n",
    "    cutoff_date = latest_date - pd.DateOffset(months=6)\n",
    "    \n",
    "    # Calculate MAE excluding last 6 months\n",
    "    mae_results = {}\n",
    "    for model in grouped_data['model'].unique():\n",
    "        model_data = grouped_data[grouped_data['model'] == model]\n",
    "        mae_data = model_data[model_data['valuation_date'] <= cutoff_date]\n",
    "        if not mae_data.empty:\n",
    "            mae = abs(mae_data['actual'] - mae_data['predicted']).mean()\n",
    "            mae_results[model] = mae\n",
    "    \n",
    "    # Create a subplot with 2 rows - one for the chart and one for the table\n",
    "    # Increase vertical spacing for more padding between chart and table\n",
    "    fig = make_subplots(\n",
    "        rows=2, \n",
    "        cols=1,\n",
    "        row_heights=[0.75, 0.25],  # Adjusted to give more space to the table\n",
    "        vertical_spacing=0.15,      # Increased for more padding\n",
    "        specs=[[{\"type\": \"scatter\"}], [{\"type\": \"table\"}]]\n",
    "    )\n",
    "    \n",
    "    # Add actual values trace to the first subplot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=actual_values['valuation_date'], \n",
    "            y=actual_values['actual'], \n",
    "            mode='lines+markers', \n",
    "            name='Actual',\n",
    "            line=dict(color='black', width=2),\n",
    "            hovertemplate='%{x}<br>Actual: %{y:.2f}'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Define color mapping for consistent colors across models\n",
    "    model_colors = {\n",
    "        'chainladder': 'blue',\n",
    "        'bornhuetter-ferguson': 'red',\n",
    "        'cape-cod': 'green',\n",
    "        'munich': 'purple'\n",
    "    }\n",
    "    \n",
    "    # Add predicted values traces for each model to the first subplot\n",
    "    for model in grouped_data['model'].unique():\n",
    "        model_data = grouped_data[grouped_data['model'] == model]\n",
    "        color = model_colors.get(model, None)  # Get color from mapping if available\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_data['valuation_date'], \n",
    "                y=model_data['predicted'], \n",
    "                mode='lines+markers', \n",
    "                name=f'ibnr - {model}',\n",
    "                line=dict(color=color) if color else {},  # Apply color if defined\n",
    "                hovertemplate='%{x}<br>Predicted: %{y:.2f}'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Build title with filtering information\n",
    "    title_parts = [f'Claim {mode.capitalize()}: Actual vs Predicted']\n",
    "    if risk_class:\n",
    "        title_parts.append(f'Risk Class: {risk_class}')\n",
    "    if channel:\n",
    "        title_parts.append(f'Channel: {channel}')\n",
    "    if prem_class:\n",
    "        title_parts.append(f'Premium Class: {prem_class}')\n",
    "    if claim_type:\n",
    "        title_parts.append(f'Claim Type: {claim_type}')\n",
    "    title = ' | '.join(title_parts)\n",
    "\n",
    "    # Create a table for MAE results, rankings and filters\n",
    "    # Sort models by MAE to determine ranking\n",
    "    ranked_models = sorted([(model, mae) for model, mae in mae_results.items()], key=lambda x: x[1])\n",
    "    \n",
    "    # Create ordered lists for the table\n",
    "    model_names = [model for model, _ in ranked_models]\n",
    "    mae_values = [f\"{mae:.2f}\" for _, mae in ranked_models]\n",
    "    # Create rankings (1 to n)\n",
    "    rankings = [f\"#{i+1}\" for i in range(len(ranked_models))]\n",
    "    \n",
    "    # Prepare filter information for the table\n",
    "    filter_names = []\n",
    "    filter_values = []\n",
    "    \n",
    "    # Add mode to filter info\n",
    "    filter_names.append(\"Mode\")\n",
    "    filter_values.append(mode)\n",
    "    \n",
    "    # Add other filters if they're set\n",
    "    if risk_class:\n",
    "        filter_names.append(\"Risk Class\")\n",
    "        filter_values.append(risk_class)\n",
    "    if channel:\n",
    "        filter_names.append(\"Channel\")\n",
    "        filter_values.append(channel)\n",
    "    if prem_class:\n",
    "        filter_names.append(\"Premium Class\")\n",
    "        filter_values.append(prem_class)\n",
    "    if claim_type:\n",
    "        filter_names.append(\"Claim Type\")\n",
    "        filter_values.append(claim_type)\n",
    "    if models:\n",
    "        filter_names.append(\"Models\")\n",
    "        filter_values.append(\", \".join(models) if isinstance(models, list) else models)\n",
    "    \n",
    "    # Add cutoff date information\n",
    "    filter_names.append(\"MAE Cutoff Date\")\n",
    "    filter_values.append(cutoff_date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    # Add MAE table to the second subplot - now including ranking column\n",
    "    fig.add_trace(\n",
    "        go.Table(\n",
    "            header=dict(\n",
    "                values=['Ranking', 'Model', 'MAE (excluding last 6 months)', 'Filter', 'Value'],\n",
    "                fill_color='paleturquoise',\n",
    "                align='left',\n",
    "                font=dict(size=12)\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[\n",
    "                    rankings + [\"\"] * (len(filter_names) - len(rankings)) if len(filter_names) > len(rankings) else rankings,\n",
    "                    model_names + [\"\"] * (len(filter_names) - len(model_names)) if len(filter_names) > len(model_names) else model_names,\n",
    "                    mae_values + [\"\"] * (len(filter_names) - len(mae_values)) if len(filter_names) > len(mae_values) else mae_values,\n",
    "                    filter_names + [\"\"] * (len(model_names) - len(filter_names)) if len(model_names) > len(filter_names) else filter_names,\n",
    "                    filter_values + [\"\"] * (len(model_names) - len(filter_values)) if len(model_names) > len(filter_values) else filter_values\n",
    "                ],\n",
    "                fill_color=[\n",
    "                    ['lavender'] * len(rankings) + ['white'] * (len(filter_names) - len(rankings)) if len(filter_names) > len(rankings) else ['lavender'] * len(rankings),\n",
    "                    ['lavender'] * len(model_names) + ['white'] * (len(filter_names) - len(model_names)) if len(filter_names) > len(model_names) else ['lavender'] * len(model_names),\n",
    "                    ['lavender'] * len(mae_values) + ['white'] * (len(filter_names) - len(mae_values)) if len(filter_names) > len(mae_values) else ['lavender'] * len(mae_values),\n",
    "                    ['ghostwhite'] * len(filter_names) + ['white'] * (len(model_names) - len(filter_names)) if len(model_names) > len(filter_names) else ['ghostwhite'] * len(filter_names),\n",
    "                    ['ghostwhite'] * len(filter_values) + ['white'] * (len(model_names) - len(filter_values)) if len(model_names) > len(filter_values) else ['ghostwhite'] * len(filter_values)\n",
    "                ],\n",
    "                align='left',\n",
    "                font=dict(size=11)\n",
    "            )\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Add zero line to the first subplot\n",
    "    min_date = actual_values['valuation_date'].min()\n",
    "    max_date = actual_values['valuation_date'].max()\n",
    "    \n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=min_date,\n",
    "        y0=0,\n",
    "        x1=max_date,\n",
    "        y1=0,\n",
    "        line=dict(color=\"gray\", width=1, dash=\"dash\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout with more height and adjusted margins\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=800,  # Increased height to better accommodate the expanded table\n",
    "        margin=dict(t=100, b=50, l=50, r=50),\n",
    "        template='plotly_white',\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update xaxis and yaxis for the first subplot\n",
    "    fig.update_xaxes(\n",
    "        title_text='Valuation Date',\n",
    "        tickangle=-45,\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(\n",
    "        title_text=f'Claim {mode.capitalize()}',\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add annotation for best model at the bottom left of the chart\n",
    "    if len(mae_results) > 1:\n",
    "        best_model = ranked_models[0][0]\n",
    "        best_mae = ranked_models[0][1]\n",
    "        fig.add_annotation(\n",
    "            x=0.07,  # Position at the bottom left (%)\n",
    "            y=0.45,  # Position at the bottom left (%)\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            text=f\"Best model: {best_model} (MAE: {best_mae:.2f})\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=11, color=\"green\"),\n",
    "            align=\"left\",\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.8)\",  # Semi-transparent white background\n",
    "            bordercolor=\"green\",\n",
    "            borderwidth=1,\n",
    "            borderpad=4,\n",
    "            xanchor=\"left\",\n",
    "            yanchor=\"bottom\"\n",
    "        )\n",
    "\n",
    "    # fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "748489f0-17e1-461e-8ec7-a09f988a7872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.2 - Claim Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f374ba4-fb37-4ada-bbb6-2536687238bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get unique channels from the DataFrame\n",
    "channels_list = result[\"channel\"].dropna().unique()\n",
    "\n",
    "# Collect all figures\n",
    "claim_count_figures = []\n",
    "\n",
    "# Loop through each channel and plot claim count\n",
    "for channel in channels_list:\n",
    "    fig = plot_claims_AVE(\n",
    "        result,\n",
    "        mode='count',\n",
    "        channel=channel\n",
    "    )\n",
    "    claim_count_figures.append((channel, fig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b341472-f992-4863-b8c3-6269f80abbe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.3 - Net Incurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "373b05cc-e061-416a-bdbb-abaecf54080f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get unique channels from the DataFrame\n",
    "channels_list = result[\"channel\"].dropna().unique()\n",
    "\n",
    "# Collect all figures\n",
    "net_incurred_figures = []\n",
    "\n",
    "# Loop through each channel and plot net incurred\n",
    "for channel in channels_list:\n",
    "    fig = plot_claims_AVE(\n",
    "        result,\n",
    "        mode='net_incurred',\n",
    "        channel=channel\n",
    "    )\n",
    "    net_incurred_figures.append((channel, fig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66368161-8986-44e1-b7f9-2228dd096205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.4 - Create HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e95a85-e29f-48a3-a174-d60b79214b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define output directory and ensure it exists\n",
    "output_dir = \"/Workspace/Shared/General/IBNR project/ibnr_modelling/outputs/claim_count_tabs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Helper function to generate tabbed HTML sections\n",
    "def generate_tab_section(figures, section_title, section_id_prefix):\n",
    "    tabs_html = []\n",
    "    buttons_html = []\n",
    "\n",
    "    for i, (channel, fig) in enumerate(figures):\n",
    "        chart_id = f\"{section_id_prefix}_chart_{i}\"\n",
    "        chart_path = f\"{output_dir}/{chart_id}.html\"\n",
    "        fig.write_html(chart_path, include_plotlyjs=False, full_html=False, div_id=chart_id)\n",
    "\n",
    "        with open(chart_path, \"r\") as f:\n",
    "            chart_div = f.read()\n",
    "\n",
    "        tabs_html.append(f'<div id=\"{chart_id}_container\" class=\"tabcontent\" style=\"display:{\"block\" if i==0 else \"none\"};\">{chart_div}</div>')\n",
    "        buttons_html.append(f'<button class=\"tablinks\" onclick=\"openTab(event, \\'{chart_id}_container\\')\">{channel}</button>')\n",
    "\n",
    "    section_html = f\"\"\"\n",
    "    <h2>{section_title}</h2>\n",
    "    <div class=\"tab\">\n",
    "      {''.join(buttons_html)}\n",
    "    </div>\n",
    "    {''.join(tabs_html)}\n",
    "    \"\"\"\n",
    "    return section_html\n",
    "\n",
    "# Generate HTML sections for claim count and net incurred figures\n",
    "claim_count_section = generate_tab_section(claim_count_figures, \"IBNR Claim Count AVE By Channel\", \"claim_count\")\n",
    "net_incurred_section = generate_tab_section(net_incurred_figures, \"IBNR Net Incurred AVE By Channel\", \"net_incurred\")\n",
    "\n",
    "# Combine everything into a full HTML\n",
    "full_html = f\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    ".tab {{\n",
    "  overflow: hidden;\n",
    "  border-bottom: 1px solid #ccc;\n",
    "}}\n",
    ".tab button {{\n",
    "  background-color: #f1f1f1;\n",
    "  float: left;\n",
    "  border: none;\n",
    "  outline: none;\n",
    "  cursor: pointer;\n",
    "  padding: 10px 16px;\n",
    "  transition: 0.3s;\n",
    "  font-size: 14px;\n",
    "}}\n",
    ".tab button:hover {{\n",
    "  background-color: #ddd;\n",
    "}}\n",
    ".tab button.active {{\n",
    "  background-color: #ccc;\n",
    "}}\n",
    ".tabcontent {{\n",
    "  display: none;\n",
    "  padding: 10px;\n",
    "  border-top: none;\n",
    "}}\n",
    "</style>\n",
    "<script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "<script>\n",
    "function openTab(evt, tabId) {{\n",
    "  var i, tabcontent, tablinks;\n",
    "  tabcontent = document.getElementsByClassName(\"tabcontent\");\n",
    "  for (i = 0; i < tabcontent.length; i++) {{\n",
    "    tabcontent[i].style.display = \"none\";\n",
    "  }}\n",
    "  tablinks = document.getElementsByClassName(\"tablinks\");\n",
    "  for (i = 0; i < tablinks.length; i++) {{\n",
    "    tablinks[i].className = tablinks[i].className.replace(\" active\", \"\");\n",
    "  }}\n",
    "  var tab = document.getElementById(tabId);\n",
    "  tab.style.display = \"block\";\n",
    "  evt.currentTarget.className += \" active\";\n",
    "\n",
    "  var plots = tab.getElementsByClassName(\"plotly-graph-div\");\n",
    "  for (var j = 0; j < plots.length; j++) {{\n",
    "      Plotly.Plots.resize(plots[j]);\n",
    "  }}\n",
    "}}\n",
    "</script>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "{claim_count_section}\n",
    "<hr>\n",
    "{net_incurred_section}\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save the final HTML file\n",
    "final_html_path = \"/Workspace/Shared/General/IBNR project/ibnr_modelling/temp_output.html\"\n",
    "with open(final_html_path, \"w\") as f:\n",
    "    f.write(full_html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5422d0af-40f9-485e-88bc-70267e2a8032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the HTML file content\n",
    "with open(\"/Workspace/Shared/General/IBNR project/ibnr_modelling/temp_output.html\", \"r\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "# Display it in the notebook\n",
    "displayHTML(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57929c5f-fad2-4967-97cf-48ffca5f8f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Ultimates Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdfd9adb-2203-4db3-9faf-85a8c7ccb187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.1 - Best Method Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14730c51-b996-4f59-8b6e-768529b59475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Union best method dataframes to be used for subsetting\n",
    "best_method_union = pd.concat([claim_count_best_method, net_incurred_best_method]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Get best model results\n",
    "bm_results = result.merge(\n",
    "    best_method_union[['channel', 'model', 'response']],\n",
    "    on=['channel', 'model', 'response'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Retrieve latest valuation date for selected models\n",
    "bm_results = bm_results[bm_results['valuation_date'] == latest_balance_date_str]\n",
    "bm_results\n",
    "\n",
    "# Check unique combinations of channel, response, and model\n",
    "# unique_combinations = filtered_results[['channel', 'response', 'model']].drop_duplicates()\n",
    "# unique_combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d0962b-da7f-4338-8f9d-3a55547c15e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.2 - Transform Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d87d3b2c-2cda-4fbb-bfe9-794e0409902d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.2.1 - Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48a8681-c659-4cfa-893a-37f3b5b98b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge raw_actual with output_combined\n",
    "ultimates_pre = pd.merge(\n",
    "    raw_data.groupby(['acc_month', 'channel', 'claim_type'])[['claim_count', 'net_claim_incurred']].sum().reset_index(),\n",
    "    bm_results,\n",
    "    on=['acc_month', 'channel', 'claim_type'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Merge with expo_data\n",
    "ultimates_pre = pd.merge(ultimates_pre, expo_data, on=['acc_month', 'channel'])\n",
    "\n",
    "# Fill missing values and convert to int64 for relevant columns\n",
    "columns_to_fix = ['net_claim_incurred', 'claim_count', 'predicted',  'exposure']\n",
    "ultimates_pre[columns_to_fix] = ultimates_pre[columns_to_fix].fillna(0).astype('int64')\n",
    "\n",
    "ultimates_pre['product'] = product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bef12fe-0ad2-4f97-b400-7095f97d59c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.2.2 - Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c5e2d0c-6c89-4f27-995c-427ad5770f2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pivoting\n",
    "pivot_indexes = ['acc_month', 'product', 'channel', 'claim_type', 'claim_count', 'net_claim_incurred', 'earnprem', 'exposure']\n",
    "\n",
    "# Pivot the predicted values\n",
    "predicted_pivot = ultimates_pre.pivot_table(index=pivot_indexes, \n",
    "                                 columns='response', \n",
    "                                 values='predicted').reset_index()\n",
    "predicted_pivot.columns.name = None\n",
    "predicted_pivot = predicted_pivot.rename(columns={\n",
    "    'count': 'ibnr_count',\n",
    "    'net_incurred': 'ibnr_incurred'\n",
    "})\n",
    "\n",
    "# Pivot the model values\n",
    "model_pivot = ultimates_pre.pivot_table(index=pivot_indexes, \n",
    "                             columns='response', \n",
    "                             values='model', aggfunc='first').reset_index()\n",
    "model_pivot.columns.name = None\n",
    "model_pivot = model_pivot.rename(columns={\n",
    "    'count': 'count_model',\n",
    "    'net_incurred': 'incurred_model'\n",
    "})\n",
    "\n",
    "# Merge the two pivoted DataFrames\n",
    "ultimates_df = pd.merge(predicted_pivot, model_pivot, on=pivot_indexes)\n",
    "\n",
    "# Create ultimate count and ultimate incurred columns\n",
    "ultimates_df['ultimate_count'] = ultimates_df['claim_count'] + ultimates_df['ibnr_count']\n",
    "ultimates_df['ultimate_incurred'] = ultimates_df['net_claim_incurred'] + ultimates_df['ibnr_incurred']\n",
    "\n",
    "display(ultimates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248a0dab-6785-4b74-a2fe-b17f576e23b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.2.3 - Inflation Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8f86c38-88fe-4491-8c2b-62461d8368eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cpi_by_quarter = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        Date,\n",
    "        CPI as cpi,\n",
    "        CONCAT(YEAR(Date), 'Q', QUARTER(Date)) AS quarter\n",
    "    FROM actuaries_prd.reference_data.abs_quarterly_cpi\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35d4383d-70c7-4390-b724-e6c8ce843845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df = ultimates_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4756e06d-2144-451d-88f6-453de98b1ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add Quarter to Ultimates Table\n",
    "test_df['quarter'] = test_df['acc_month'].dt.to_period('Q').astype(str).str.replace('Q', 'Q', regex=False)\n",
    "\n",
    "# CPI\n",
    "ultimates_with_cpi = test_df.merge(cpi_by_quarter[['quarter', 'cpi']], on='quarter', how='left')\n",
    "\n",
    "# Base CPI\n",
    "base_cpi_quarter = sorted(ultimates_with_cpi['quarter'].unique())[-2] # Get second-last quarter in dataset\n",
    "base_cpi_value = cpi_by_quarter.loc[cpi_by_quarter['quarter'] == base_cpi_quarter, 'cpi'].values[0]\n",
    "ultimates_with_cpi['base_cpi'] = base_cpi_value\n",
    "\n",
    "# Index Multiplier\n",
    "ultimates_with_cpi['index_multiplier'] = ultimates_with_cpi['base_cpi'] / ultimates_with_cpi['cpi']\n",
    "\n",
    "# Adjusted Incurreds\n",
    "ultimates_with_cpi['adj_ultimate_incurred'] = ultimates_with_cpi['ultimate_incurred'] * ultimates_with_cpi['index_multiplier']\n",
    "ultimates_with_cpi['adj_net_claim_incurred'] = ultimates_with_cpi['net_claim_incurred'] * ultimates_with_cpi['index_multiplier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3f9dc3-936a-45d9-99c0-b799de51219a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ultimates_with_cpi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8659c0b4-a4dc-48bc-9e8b-e5067650046c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.2.4 - Update Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e3b0b37-d912-49f2-a603-02e549313a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save ultimates data down\n",
    "# spark.createDataFrame(ultimates_df).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"actuaries_prd.general.pm_ultimates_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c755ff93-eb6e-4542-8b96-3dfe7f0755d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save ultimates data down\n",
    "spark.createDataFrame(ultimates_with_cpi).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"actuaries_prd.general.pm_ultimates_new\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IBNR Modelling Template (PM)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
